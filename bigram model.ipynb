{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import random\n",
    "import pickle\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "from gensim.models.phrases import Phraser\n",
    "from gensim.models import Word2Vec, phrases \n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed text_1971-1980_American Journal of Political Science.txt: 354 articles sampled.\n",
      "Processed text_1971-1980_British Journal of Political Science.txt: 244 articles sampled.\n",
      "Processed text_1981-1990_American Journal of Political Science.txt: 326 articles sampled.\n",
      "Processed text_1981-1990_British Journal of Political Science.txt: 198 articles sampled.\n",
      "Processed text_1991-2000_American Journal of Political Science.txt: 404 articles sampled.\n",
      "Processed text_1991-2000_British Journal of Political Science.txt: 209 articles sampled.\n",
      "Processed text_2001-2010_American Journal of Political Science.txt: 445 articles sampled.\n",
      "Processed text_2001-2010_British Journal of Political Science.txt: 397 articles sampled.\n",
      "Processed text_2011-2020_American Journal of Political Science.txt: 932 articles sampled.\n",
      "Processed text_2011-2020_British Journal of Political Science.txt: 617 articles sampled.\n",
      "Processed text_2021-2024_American Journal of Political Science.txt: 270 articles sampled.\n",
      "Processed text_2021-2024_British Journal of Political Science.txt: 236 articles sampled.\n",
      "                                            sentence\n",
      "0  [richard, born, kansas, state, christopher, ne...\n",
      "1  [straightforward, mathematical, manipulation, ...\n",
      "2  [part, efforts, develop, operational, measures...\n",
      "3                                             [ibid]\n",
      "4  [stuart, rice, index, cohesion, first, develop...\n"
     ]
    }
   ],
   "source": [
    "### bigram tranformer\n",
    "## the sample text for bigram model\n",
    "import os\n",
    "import random\n",
    "import pandas as pd\n",
    "\n",
    "def process_subcorpora(input_dir, sample_ratio=0.75, delimiter=' SENTENCESPLITHERE '):\n",
    "    \"\"\"\n",
    "    Process text files representing subcorpora by:\n",
    "    - Loading the text data\n",
    "    - Sampling a fraction of the articles\n",
    "    - Splitting them into sentences\n",
    "    - Storing processed sentences separately for each file\n",
    "\n",
    "    Parameters:\n",
    "        input_dir (str): Directory containing the subcorpora text files.\n",
    "        sample_ratio (float): Fraction of articles to sample (default: 0.75).\n",
    "        delimiter (str): Sentence boundary delimiter used in the text.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary where keys are filenames (without .txt) and values are lists of tokenized sentences.\n",
    "    \"\"\"\n",
    "\n",
    "    processed_data = {}\n",
    "\n",
    "    # Iterate over all text files in the directory\n",
    "    for filename in sorted(os.listdir(input_dir)):\n",
    "        if filename.endswith(\".txt\"):  \n",
    "            file_path = os.path.join(input_dir, filename)\n",
    "            file_key = filename.replace(\".txt\", \"\")  # Remove .txt for dictionary key\n",
    "            \n",
    "            # Read the file\n",
    "            with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "                articles = file.readlines()  # Each line represents an article\n",
    "            \n",
    "            # Sample a fraction of the articles\n",
    "            sample_size = round(sample_ratio * len(articles))\n",
    "            sampled_articles = random.sample(articles, sample_size) if articles else []\n",
    "            \n",
    "            # Process each sampled article\n",
    "            tokenized_sentences = []\n",
    "            for article in sampled_articles:\n",
    "                sentences_list = article.split(delimiter)\n",
    "                for sentence in sentences_list:\n",
    "                    sentence_tokens = sentence.split()  # Tokenize by whitespace\n",
    "                    if sentence_tokens:\n",
    "                        tokenized_sentences.append(sentence_tokens)  # Store tokenized sentence\n",
    "\n",
    "            processed_data[file_key] = tokenized_sentences  # Store in dictionary\n",
    "\n",
    "            print(f\"Processed {filename}: {len(sampled_articles)} articles sampled.\")\n",
    "\n",
    "    return processed_data\n",
    "\n",
    "# Example usage:\n",
    "input_directory = '/Users/yvette/Desktop/data/Final/preprocessed grouped txt'\n",
    "processed_sentences_dict = process_subcorpora(input_directory)\n",
    "\n",
    "# Convert each file's processed data into a DataFrame with a single \"sentence\" column\n",
    "dfs = {file: pd.DataFrame({\"sentence\": sentences}) for file, sentences in processed_sentences_dict.items()}\n",
    "\n",
    "# Example: Access DataFrame for a specific file\n",
    "sample_filename = list(dfs.keys())[0]  # Get first filename\n",
    "print(dfs[sample_filename].head())  # Show first few rows of its DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: /Users/yvette/Desktop/data/Final/sample text for phraser/text_1971-1980_American Journal of Political Science.csv\n",
      "Saved: /Users/yvette/Desktop/data/Final/sample text for phraser/text_1971-1980_British Journal of Political Science.csv\n",
      "Saved: /Users/yvette/Desktop/data/Final/sample text for phraser/text_1981-1990_American Journal of Political Science.csv\n",
      "Saved: /Users/yvette/Desktop/data/Final/sample text for phraser/text_1981-1990_British Journal of Political Science.csv\n",
      "Saved: /Users/yvette/Desktop/data/Final/sample text for phraser/text_1991-2000_American Journal of Political Science.csv\n",
      "Saved: /Users/yvette/Desktop/data/Final/sample text for phraser/text_1991-2000_British Journal of Political Science.csv\n",
      "Saved: /Users/yvette/Desktop/data/Final/sample text for phraser/text_2001-2010_American Journal of Political Science.csv\n",
      "Saved: /Users/yvette/Desktop/data/Final/sample text for phraser/text_2001-2010_British Journal of Political Science.csv\n",
      "Saved: /Users/yvette/Desktop/data/Final/sample text for phraser/text_2011-2020_American Journal of Political Science.csv\n",
      "Saved: /Users/yvette/Desktop/data/Final/sample text for phraser/text_2011-2020_British Journal of Political Science.csv\n",
      "Saved: /Users/yvette/Desktop/data/Final/sample text for phraser/text_2021-2024_American Journal of Political Science.csv\n",
      "Saved: /Users/yvette/Desktop/data/Final/sample text for phraser/text_2021-2024_British Journal of Political Science.csv\n"
     ]
    }
   ],
   "source": [
    "output_dir = \"/Users/yvette/Desktop/data/Final/sample text for phraser\"\n",
    "\n",
    "# Ensure the directory exists\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "for filename, df in dfs.items():\n",
    "    file_path = os.path.join(output_dir, f\"{filename}.csv\")\n",
    "    df.to_csv(file_path, index=False)\n",
    "    print(f\"Saved: {file_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the function to train and evaluate bigram models using processed sentences\n",
    "def evaluate_bigram_model(model, processed_sentences_dict, specific_filename, bigram_dir):\n",
    "    detected_bigrams = set()\n",
    "\n",
    "    # Apply the bigram model to the specific file in processed_sentences_dict\n",
    "    sentences = processed_sentences_dict.get(specific_filename, [])\n",
    "    \n",
    "    if not sentences:\n",
    "        print(f\"No sentences found for {specific_filename}.\")\n",
    "        return None\n",
    "    \n",
    "    # Debugging - check phraser type\n",
    "    print(f\"Phaser model for {specific_filename}: {type(model)}\")\n",
    "\n",
    "    # Apply the model to each sentence and collect bigrams\n",
    "    for sentence in sentences:\n",
    "        transformed_sentence = model[sentence]  # Apply the bigram model (Phraser)\n",
    "        for bigram in transformed_sentence:\n",
    "            if isinstance(bigram, tuple):  # Check if it's a tuple (e.g., (\"word1\", \"word2\"))\n",
    "                detected_bigrams.add(\"_\".join(bigram))  # Join the words with \"_\"\n",
    "            elif isinstance(bigram, str):  # If it's a string, assume it's already a bigram\n",
    "                detected_bigrams.add(bigram)\n",
    "\n",
    "    # Extract the common part of the filename (before the first underscore)\n",
    "    common_part = \"_\".join(specific_filename.split(\"_\")[1:])  # Skip 'text_' part, keep everything after it\n",
    "    print(f\"Common part of the filename: {common_part}\")\n",
    "    \n",
    "    # Dynamically generate the reference filename using the common part\n",
    "    reference_filename = f\"bigram_{common_part}_without_stopword.csv\"\n",
    "    reference_file_path = os.path.join(bigram_dir, reference_filename)\n",
    "    \n",
    "    if not os.path.exists(reference_file_path):\n",
    "        print(f\"Reference bigram file for {specific_filename} not found.\")\n",
    "        return None\n",
    "\n",
    "    # Load the reference bigram DataFrame\n",
    "    bigram_df = pd.read_csv(reference_file_path)\n",
    "\n",
    "    # Extract bigrams from the 'ngram' column and format them consistently\n",
    "    reference_bigrams = set(bigram_df[\"ngram\"].astype(str).apply(lambda x: x.replace(\" \", \"_\")))\n",
    "\n",
    "    # Compute Precision, Recall, and F1-score\n",
    "    intersection = detected_bigrams.intersection(reference_bigrams)\n",
    "    precision = len(intersection) / len(detected_bigrams) if detected_bigrams else 0\n",
    "    recall = len(intersection) / len(reference_bigrams) if reference_bigrams else 0\n",
    "    f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "\n",
    "    return precision, recall, f1\n",
    "\n",
    "# Define a DataFrame to store the results\n",
    "results_df = pd.DataFrame(columns=[\"min_count\", \"threshold\", \"filename\", \"Precision\", \"Recall\", \"F1\"])\n",
    "# Loop through different parameter combinations for min_count and threshold\n",
    "def evaluate_for_all_files(processed_sentences_dict, bigram_dir):\n",
    "    for min_count in [10, 20, 30, 40]:\n",
    "        for threshold in [5, 8, 10, 12, 15]:\n",
    "            \n",
    "            # Loop through all the filenames in processed_sentences_dict\n",
    "            for specific_filename in processed_sentences_dict.keys():\n",
    "                print(f\"Evaluating bigram model for {specific_filename}...\")\n",
    "                \n",
    "                # Train the bigram model with the current parameters\n",
    "                bigram_model = phrases.Phrases(processed_sentences_dict[specific_filename], min_count=min_count, threshold=threshold)\n",
    "                \n",
    "                # Convert to Phraser for optimized speed\n",
    "                phraser = Phraser(bigram_model)\n",
    "                \n",
    "                # Evaluate the model using precision, recall, and F1 score for the specific file\n",
    "                precision, recall, f1 = evaluate_bigram_model(phraser, processed_sentences_dict, specific_filename, bigram_dir)\n",
    "        \n",
    "\n",
    "                # Print the results\n",
    "                if precision is not None:\n",
    "                    print(f\"min_count={min_count}, threshold={threshold}, filename={specific_filename} => Precision: {precision}, Recall: {recall}, F1: {f1}\")\n",
    "                    # Store the results in the DataFrame\n",
    "                    results_df.loc[len(results_df)] = [min_count, threshold, specific_filename, precision, recall, f1]\n",
    "\n",
    "    return results_df  \n",
    "\n",
    "\n",
    "\n",
    "bigram_dir = '/Users/yvette/Desktop/data/Final/ngram word frequency without stopwords'\n",
    "evaluate_for_all_files(processed_sentences_dict, bigram_dir)\n",
    "results_df.to_csv(\"/Users/yvette/Desktop/bigram_results.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    min_count  threshold  Precision    Recall        F1\n",
      "0          10          5   0.108347  0.012104  0.021753\n",
      "1          10          8   0.096650  0.010589  0.019067\n",
      "2          10         10   0.091025  0.009884  0.017812\n",
      "3          10         12   0.086431  0.009317  0.016803\n",
      "4          10         15   0.080946  0.008653  0.015617\n",
      "5          20          5   0.051402  0.005268  0.009547\n",
      "6          20          8   0.045707  0.004641  0.008417\n",
      "7          20         10   0.042849  0.004332  0.007860\n",
      "8          20         12   0.040606  0.004091  0.007426\n",
      "9          20         15   0.037839  0.003797  0.006894\n",
      "10         30          5   0.031353  0.003118  0.005667\n",
      "11         30          8   0.027858  0.002755  0.005009\n",
      "12         30         10   0.026103  0.002575  0.004682\n",
      "13         30         12   0.024672  0.002428  0.004416\n",
      "14         30         15   0.022914  0.002249  0.004091\n",
      "15         40          5   0.021495  0.002106  0.003833\n",
      "16         40          8   0.019019  0.001856  0.003378\n",
      "17         40         10   0.017771  0.001731  0.003152\n",
      "18         40         12   0.016753  0.001629  0.002966\n",
      "19         40         15   0.015558  0.001510  0.002749\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the CSV data\n",
    "df = pd.read_csv(\"/Users/yvette/Desktop/data/Final/bigram_results.csv\")\n",
    "\n",
    "# Group by 'min_count' and 'threshold', calculate mean of metrics\n",
    "grouped = df.groupby(['min_count', 'threshold']).agg({\n",
    "    'Precision': 'mean',\n",
    "    'Recall': 'mean',\n",
    "    'F1': 'mean'\n",
    "}).reset_index()\n",
    "\n",
    "# Sort by F1 in descending order\n",
    "sorted_result = grouped.sort_values('F1', ascending=False)\n",
    "\n",
    "# Print or save the result\n",
    "print(sorted_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bigram model for text_1971-1980_American Journal of Political Science...\n",
      "Bigram model saved at: /Users/yvette/Desktop/data/Final/bigram phraser/bigram_text_1971-1980_American Journal of Political Science.model\n",
      "bigram model for text_1971-1980_British Journal of Political Science...\n",
      "Bigram model saved at: /Users/yvette/Desktop/data/Final/bigram phraser/bigram_text_1971-1980_British Journal of Political Science.model\n",
      "bigram model for text_1981-1990_American Journal of Political Science...\n",
      "Bigram model saved at: /Users/yvette/Desktop/data/Final/bigram phraser/bigram_text_1981-1990_American Journal of Political Science.model\n",
      "bigram model for text_1981-1990_British Journal of Political Science...\n",
      "Bigram model saved at: /Users/yvette/Desktop/data/Final/bigram phraser/bigram_text_1981-1990_British Journal of Political Science.model\n",
      "bigram model for text_1991-2000_American Journal of Political Science...\n",
      "Bigram model saved at: /Users/yvette/Desktop/data/Final/bigram phraser/bigram_text_1991-2000_American Journal of Political Science.model\n",
      "bigram model for text_1991-2000_British Journal of Political Science...\n",
      "Bigram model saved at: /Users/yvette/Desktop/data/Final/bigram phraser/bigram_text_1991-2000_British Journal of Political Science.model\n",
      "bigram model for text_2001-2010_American Journal of Political Science...\n",
      "Bigram model saved at: /Users/yvette/Desktop/data/Final/bigram phraser/bigram_text_2001-2010_American Journal of Political Science.model\n",
      "bigram model for text_2001-2010_British Journal of Political Science...\n",
      "Bigram model saved at: /Users/yvette/Desktop/data/Final/bigram phraser/bigram_text_2001-2010_British Journal of Political Science.model\n",
      "bigram model for text_2011-2020_American Journal of Political Science...\n",
      "Bigram model saved at: /Users/yvette/Desktop/data/Final/bigram phraser/bigram_text_2011-2020_American Journal of Political Science.model\n",
      "bigram model for text_2011-2020_British Journal of Political Science...\n",
      "Bigram model saved at: /Users/yvette/Desktop/data/Final/bigram phraser/bigram_text_2011-2020_British Journal of Political Science.model\n",
      "bigram model for text_2021-2024_American Journal of Political Science...\n",
      "Bigram model saved at: /Users/yvette/Desktop/data/Final/bigram phraser/bigram_text_2021-2024_American Journal of Political Science.model\n",
      "bigram model for text_2021-2024_British Journal of Political Science...\n",
      "Bigram model saved at: /Users/yvette/Desktop/data/Final/bigram phraser/bigram_text_2021-2024_British Journal of Political Science.model\n"
     ]
    }
   ],
   "source": [
    "### bigram model choosed （min_count 10, threshold 8）and saved \n",
    "bigram_save_dir = \"/Users/yvette/Desktop/data/Final/bigram phraser\"\n",
    "\n",
    "for specific_filename in processed_sentences_dict.keys():\n",
    "    print(f\"bigram model for {specific_filename}...\")\n",
    "                \n",
    "    # Train the bigram model with the current parameters\n",
    "    bigram_model = phrases.Phrases(processed_sentences_dict[specific_filename], min_count=10, threshold=8)\n",
    "\n",
    "    # save bigram phraser\n",
    "    bigram_model_path = os.path.join(bigram_save_dir, f\"bigram_{specific_filename}.model\")\n",
    "    bigram_phraser = Phraser(bigram_model)\n",
    "    bigram_model.save(bigram_model_path)\n",
    "\n",
    "    print(f\"Bigram model saved at: {bigram_model_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed text_1971-1980_American Journal of Political Science.txt: 472 articles processed.\n",
      "Processed text_1971-1980_British Journal of Political Science.txt: 326 articles processed.\n",
      "Processed text_1981-1990_American Journal of Political Science.txt: 435 articles processed.\n",
      "Processed text_1981-1990_British Journal of Political Science.txt: 264 articles processed.\n",
      "Processed text_1991-2000_American Journal of Political Science.txt: 538 articles processed.\n",
      "Processed text_1991-2000_British Journal of Political Science.txt: 279 articles processed.\n",
      "Processed text_2001-2010_American Journal of Political Science.txt: 593 articles processed.\n",
      "Processed text_2001-2010_British Journal of Political Science.txt: 530 articles processed.\n",
      "Processed text_2011-2020_American Journal of Political Science.txt: 1242 articles processed.\n",
      "Processed text_2011-2020_British Journal of Political Science.txt: 823 articles processed.\n",
      "Processed text_2021-2024_American Journal of Political Science.txt: 360 articles processed.\n",
      "Processed text_2021-2024_British Journal of Political Science.txt: 314 articles processed.\n",
      "                                            sentence\n",
      "0  [book, reviews, childs, construction, politics...\n",
      "1                      [melbourne, melbourne, press]\n",
      "2  [poverty, politicization, political, socializa...\n",
      "3                           [new, york, free, press]\n",
      "4  [human, basis, polity, psychological, study, p...\n",
      "bigram model for text_1971-1980_American Journal of Political Science...\n",
      "Bigram model saved at: /Users/yvette/Desktop/data/Final/bigram phraser full text/bigram_text_1971-1980_American Journal of Political Science.model\n",
      "bigram model for text_1971-1980_British Journal of Political Science...\n",
      "Bigram model saved at: /Users/yvette/Desktop/data/Final/bigram phraser full text/bigram_text_1971-1980_British Journal of Political Science.model\n",
      "bigram model for text_1981-1990_American Journal of Political Science...\n",
      "Bigram model saved at: /Users/yvette/Desktop/data/Final/bigram phraser full text/bigram_text_1981-1990_American Journal of Political Science.model\n",
      "bigram model for text_1981-1990_British Journal of Political Science...\n",
      "Bigram model saved at: /Users/yvette/Desktop/data/Final/bigram phraser full text/bigram_text_1981-1990_British Journal of Political Science.model\n",
      "bigram model for text_1991-2000_American Journal of Political Science...\n",
      "Bigram model saved at: /Users/yvette/Desktop/data/Final/bigram phraser full text/bigram_text_1991-2000_American Journal of Political Science.model\n",
      "bigram model for text_1991-2000_British Journal of Political Science...\n",
      "Bigram model saved at: /Users/yvette/Desktop/data/Final/bigram phraser full text/bigram_text_1991-2000_British Journal of Political Science.model\n",
      "bigram model for text_2001-2010_American Journal of Political Science...\n",
      "Bigram model saved at: /Users/yvette/Desktop/data/Final/bigram phraser full text/bigram_text_2001-2010_American Journal of Political Science.model\n",
      "bigram model for text_2001-2010_British Journal of Political Science...\n",
      "Bigram model saved at: /Users/yvette/Desktop/data/Final/bigram phraser full text/bigram_text_2001-2010_British Journal of Political Science.model\n",
      "bigram model for text_2011-2020_American Journal of Political Science...\n",
      "Bigram model saved at: /Users/yvette/Desktop/data/Final/bigram phraser full text/bigram_text_2011-2020_American Journal of Political Science.model\n",
      "bigram model for text_2011-2020_British Journal of Political Science...\n",
      "Bigram model saved at: /Users/yvette/Desktop/data/Final/bigram phraser full text/bigram_text_2011-2020_British Journal of Political Science.model\n",
      "bigram model for text_2021-2024_American Journal of Political Science...\n",
      "Bigram model saved at: /Users/yvette/Desktop/data/Final/bigram phraser full text/bigram_text_2021-2024_American Journal of Political Science.model\n",
      "bigram model for text_2021-2024_British Journal of Political Science...\n",
      "Bigram model saved at: /Users/yvette/Desktop/data/Final/bigram phraser full text/bigram_text_2021-2024_British Journal of Political Science.model\n",
      "Training trigram model for text_1971-1980_American Journal of Political Science...\n",
      "Trigram model saved at: /Users/yvette/Desktop/data/Final/trigram phraser full text/trigram_text_1971-1980_American Journal of Political Science.model\n",
      "Training trigram model for text_1971-1980_British Journal of Political Science...\n",
      "Trigram model saved at: /Users/yvette/Desktop/data/Final/trigram phraser full text/trigram_text_1971-1980_British Journal of Political Science.model\n",
      "Training trigram model for text_1981-1990_American Journal of Political Science...\n",
      "Trigram model saved at: /Users/yvette/Desktop/data/Final/trigram phraser full text/trigram_text_1981-1990_American Journal of Political Science.model\n",
      "Training trigram model for text_1981-1990_British Journal of Political Science...\n",
      "Trigram model saved at: /Users/yvette/Desktop/data/Final/trigram phraser full text/trigram_text_1981-1990_British Journal of Political Science.model\n",
      "Training trigram model for text_1991-2000_American Journal of Political Science...\n",
      "Trigram model saved at: /Users/yvette/Desktop/data/Final/trigram phraser full text/trigram_text_1991-2000_American Journal of Political Science.model\n",
      "Training trigram model for text_1991-2000_British Journal of Political Science...\n",
      "Trigram model saved at: /Users/yvette/Desktop/data/Final/trigram phraser full text/trigram_text_1991-2000_British Journal of Political Science.model\n",
      "Training trigram model for text_2001-2010_American Journal of Political Science...\n",
      "Trigram model saved at: /Users/yvette/Desktop/data/Final/trigram phraser full text/trigram_text_2001-2010_American Journal of Political Science.model\n",
      "Training trigram model for text_2001-2010_British Journal of Political Science...\n",
      "Trigram model saved at: /Users/yvette/Desktop/data/Final/trigram phraser full text/trigram_text_2001-2010_British Journal of Political Science.model\n",
      "Training trigram model for text_2011-2020_American Journal of Political Science...\n",
      "Trigram model saved at: /Users/yvette/Desktop/data/Final/trigram phraser full text/trigram_text_2011-2020_American Journal of Political Science.model\n",
      "Training trigram model for text_2011-2020_British Journal of Political Science...\n",
      "Trigram model saved at: /Users/yvette/Desktop/data/Final/trigram phraser full text/trigram_text_2011-2020_British Journal of Political Science.model\n",
      "Training trigram model for text_2021-2024_American Journal of Political Science...\n",
      "Trigram model saved at: /Users/yvette/Desktop/data/Final/trigram phraser full text/trigram_text_2021-2024_American Journal of Political Science.model\n",
      "Training trigram model for text_2021-2024_British Journal of Political Science...\n",
      "Trigram model saved at: /Users/yvette/Desktop/data/Final/trigram phraser full text/trigram_text_2021-2024_British Journal of Political Science.model\n"
     ]
    }
   ],
   "source": [
    "### bigram for full text \n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "def process_subcorpora(input_dir, delimiter=' SENTENCESPLITHERE '):\n",
    "    \"\"\"\n",
    "    Process text files representing subcorpora by:\n",
    "    - Loading the text data\n",
    "    - Splitting them into sentences\n",
    "    - Storing processed sentences separately for each file\n",
    "\n",
    "    Parameters:\n",
    "        input_dir (str): Directory containing the subcorpora text files.\n",
    "        delimiter (str): Sentence boundary delimiter used in the text.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary where keys are filenames (without .txt) and values are lists of tokenized sentences.\n",
    "    \"\"\"\n",
    "\n",
    "    processed_data = {}\n",
    "\n",
    "    # Iterate over all text files in the directory\n",
    "    for filename in sorted(os.listdir(input_dir)):\n",
    "        if filename.endswith(\".txt\"):  \n",
    "            file_path = os.path.join(input_dir, filename)\n",
    "            file_key = filename.replace(\".txt\", \"\")  # Remove .txt for dictionary key\n",
    "            \n",
    "            # Read the file\n",
    "            with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "                articles = file.readlines()  # Each line represents an article\n",
    "            \n",
    "            # Process each article (without sampling)\n",
    "            tokenized_sentences = []\n",
    "            for article in articles:\n",
    "                sentences_list = article.split(delimiter)\n",
    "                for sentence in sentences_list:\n",
    "                    sentence_tokens = sentence.split()  # Tokenize by whitespace\n",
    "                    if sentence_tokens:\n",
    "                        tokenized_sentences.append(sentence_tokens)  # Store tokenized sentence\n",
    "\n",
    "            processed_data[file_key] = tokenized_sentences  # Store in dictionary\n",
    "\n",
    "            print(f\"Processed {filename}: {len(articles)} articles processed.\")\n",
    "\n",
    "    return processed_data\n",
    "\n",
    "# Example usage:\n",
    "input_directory = '/Users/yvette/Desktop/data/Final/preprocessed grouped txt'\n",
    "processed_sentences_dict = process_subcorpora(input_directory)\n",
    "\n",
    "# Convert each file's processed data into a DataFrame with a single \"sentence\" column\n",
    "dfs = {file: pd.DataFrame({\"sentence\": sentences}) for file, sentences in processed_sentences_dict.items()}\n",
    "\n",
    "# Example: Access DataFrame for a specific file\n",
    "sample_filename = list(dfs.keys())[0]  # Get first filename\n",
    "print(dfs[sample_filename].head())  # Show first few rows of its DataFrame\n",
    "\n",
    "bigram_save_dir = \"/Users/yvette/Desktop/data/Final/bigram phraser full text\"\n",
    "\n",
    "for specific_filename in processed_sentences_dict.keys():\n",
    "    print(f\"bigram model for {specific_filename}...\")\n",
    "                \n",
    "    # Train the bigram model with the current parameters\n",
    "    bigram_model = phrases.Phrases(processed_sentences_dict[specific_filename], min_count=10, threshold=8)\n",
    "\n",
    "    # generate path for save\n",
    "    bigram_model_path = os.path.join(bigram_save_dir, f\"bigram_{specific_filename}.model\")\n",
    "    bigram_phraser = Phraser(bigram_model)\n",
    "    \n",
    "    # save bigram phraser\n",
    "    bigram_model.save(bigram_model_path)\n",
    "\n",
    "    print(f\"Bigram model saved at: {bigram_model_path}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
