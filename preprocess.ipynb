{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method NDFrame.head of                                         id  \\\n",
      "0                   ark://27927/pjcswvb5kf   \n",
      "1                  ark://27927/pjb7q8vz8dg   \n",
      "2                  ark://27927/phz45krzm5c   \n",
      "3                  ark://27927/phzptbf6jw3   \n",
      "4                  ark://27927/pgh2cghxgbg   \n",
      "...                                    ...   \n",
      "6169  http://www.jstor.org/stable/25193870   \n",
      "6170   http://www.jstor.org/stable/2110624   \n",
      "6171    http://www.jstor.org/stable/194108   \n",
      "6172  http://www.jstor.org/stable/25193859   \n",
      "6173   http://www.jstor.org/stable/3647727   \n",
      "\n",
      "                                                  title  \\\n",
      "0     Buttery Guns and Welfare Hawks: The Politics o...   \n",
      "1     Do Campaign Contribution Limits Curb the Influ...   \n",
      "2     When Toleration Becomes a Vice: Naming Aristot...   \n",
      "3     How Does Minority Political Representation Aff...   \n",
      "4     Assortative Mating on Ideology Could Operate T...   \n",
      "...                                                 ...   \n",
      "6169  How Should We Estimate Public Opinion in the S...   \n",
      "6170  Estimating and Interpreting Correlations betwe...   \n",
      "6171  Attitudes towards a Fallen Leader: Evaluations...   \n",
      "6172  Economic Inequality and Intolerance: Attitudes...   \n",
      "6173  Congressional Politics of International Financ...   \n",
      "\n",
      "                                   isPartOf  publicationYear  docType  \\\n",
      "0     American Journal of Political Science             2011  article   \n",
      "1     American Journal of Political Science             2022  article   \n",
      "2     American Journal of Political Science             2018  article   \n",
      "3     American Journal of Political Science             2021  article   \n",
      "4     American Journal of Political Science             2014  article   \n",
      "...                                     ...              ...      ...   \n",
      "6169  American Journal of Political Science             2009  article   \n",
      "6170  American Journal of Political Science             1978  article   \n",
      "6171   British Journal of Political Science             1996  article   \n",
      "6172  American Journal of Political Science             2008  article   \n",
      "6173  American Journal of Political Science             2005  article   \n",
      "\n",
      "            docSubType provider datePublished  issueNumber  volumeNumber  ...  \\\n",
      "0              article  portico      2011/1/1            1            55  ...   \n",
      "1              article  portico     2022/10/1            4            66  ...   \n",
      "2              article  portico     2018/10/1            4            62  ...   \n",
      "3                 None  portico      2021/7/1            3            65  ...   \n",
      "4              article  portico     2014/10/1            4            58  ...   \n",
      "...                ...      ...           ...          ...           ...  ...   \n",
      "6169  research-article    jstor    2009-01-01            1            53  ...   \n",
      "6170  research-article    jstor    1978-05-01            2            22  ...   \n",
      "6171  research-article    jstor    1996-07-01            3            26  ...   \n",
      "6172  research-article    jstor    2008-10-01            4            52  ...   \n",
      "6173  research-article    jstor    2005-07-01            3            49  ...   \n",
      "\n",
      "                                  publisher language pageStart pageEnd  \\\n",
      "0                   John Wiley & Sons, Inc.      eng     117.0   134.0   \n",
      "1                   John Wiley & Sons, Inc.      eng     932.0   946.0   \n",
      "2                   John Wiley & Sons, Inc.      eng     849.0   860.0   \n",
      "3                   John Wiley & Sons, Inc.      eng     699.0   716.0   \n",
      "4                   John Wiley & Sons, Inc.      eng     997.0  1005.0   \n",
      "...                                     ...      ...       ...     ...   \n",
      "6169  Midwest Political Science Association      eng     107.0   121.0   \n",
      "6170  Midwest Political Science Association      eng     444.0   474.0   \n",
      "6171             Cambridge University Press      eng     429.0   439.0   \n",
      "6172  Midwest Political Science Association      eng     942.0   958.0   \n",
      "6173  Midwest Political Science Association      eng     479.0   496.0   \n",
      "\n",
      "                                              keyphrase  wordCount pageCount  \\\n",
      "0     military spending; conflict involvement; gover...      13004        18   \n",
      "1     donors; looser limits; contracts; campaign; co...      13446        15   \n",
      "2     toleration; aristotle; virtue; nicomachean eth...       9893        12   \n",
      "3     minority; candidate; school; student achieveme...      11242        18   \n",
      "4     ideology; assortative mating; evaluator; dusti...       8150         9   \n",
      "...                                                 ...        ...       ...   \n",
      "6169  disaggregation, estimates, demographic, mrp es...      10960        15   \n",
      "6170  nonrecursive, disturbances, residual path, var...      11358        31   \n",
      "6171  olof palme, towards palme, attitudes towards, ...       4630        11   \n",
      "6172  gdp per, toward homosexuality, gini coefficien...      10836        17   \n",
      "6173  rescues, financial rescues, sanders, esf rescu...      12137        18   \n",
      "\n",
      "                       outputFormat  \\\n",
      "0          unigram; bigram; trigram   \n",
      "1          unigram; bigram; trigram   \n",
      "2          unigram; bigram; trigram   \n",
      "3          unigram; bigram; trigram   \n",
      "4          unigram; bigram; trigram   \n",
      "...                             ...   \n",
      "6169  [unigrams, bigrams, trigrams]   \n",
      "6170  [unigrams, bigrams, trigrams]   \n",
      "6171  [unigrams, bigrams, trigrams]   \n",
      "6172  [unigrams, bigrams, trigrams]   \n",
      "6173  [unigrams, bigrams, trigrams]   \n",
      "\n",
      "                                                   text year_range  \n",
      "0     Buttery Guns and Welfare Hawks: The Politics\\n...  2011-2020  \n",
      "1     Do Campaign Contribution Limits Curb\\nthe Inﬂu...  2021-2024  \n",
      "2     When Toleration Becomes a Vice: Naming Aristot...  2011-2020  \n",
      "3     How Does Minority Political Representation Aff...  2021-2024  \n",
      "4     Assortative Mating on Ideology Could Operate\\n...  2011-2020  \n",
      "...                                                 ...        ...  \n",
      "6169  How Should We Estimate Public Opinion in The S...  2001-2010  \n",
      "6170  THE WORKSHOP Estimating and Interpreting Corre...  1971-1980  \n",
      "6171  Notes and Comments 429 Attitudes Towards a Fal...  1991-2000  \n",
      "6172  Economie Inequality and Intolerance: Attitudes...  2001-2010  \n",
      "6173  Congressional Politics of International Financ...  2001-2010  \n",
      "\n",
      "[6174 rows x 22 columns]>\n"
     ]
    }
   ],
   "source": [
    "# Load JSONL file into a DataFrame\n",
    "def jsonl_to_dataframe(file_path):\n",
    "    return pd.read_json(file_path, lines=True)\n",
    "\n",
    "file_path = \"/Users/yvette/Desktop/data/Final/df_decade.jsonl\"\n",
    "df = jsonl_to_dataframe(file_path)\n",
    "\n",
    "# Display the first few rows\n",
    "print(df.head)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a set of custom stopwords\n",
    "custom_stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Add the words you want to keep\n",
    "custom_metadata_stopwords = {\n",
    "    \"downloaded\", \"doi\", \"wiley\", \"author\", \"email\", \"vol\", \"et\", \"al\", \"pp\", \"copyright\", \"ph.d.\", \"issue\", \"volume\", \"edition\", \n",
    "    \"university\", \"oxford\",\"cambridge\", \"b.j.pol.s.\", \"midwest\", \"linkoping\", \"creative commons\",  \"open access\"\n",
    "    # Add all months\n",
    "    \"january\", \"february\", \"march\", \"april\", \"may\", \"june\", \"july\",\n",
    "    \"august\", \"september\", \"october\", \"november\", \"december\"\n",
    "}\n",
    "\n",
    "custom_stop_words.update(custom_metadata_stopwords)\n",
    "\n",
    "# Preprocessing ngram\n",
    "def preprocess_ngram(text):\n",
    "    if not isinstance(text, str):  # Handle NaN or non-string values\n",
    "        return \"\"\n",
    "    \n",
    "    # Tokenization\n",
    "    tokens = word_tokenize(text)\n",
    "\n",
    "    # Determine n-gram type\n",
    "    num_words = len(tokens)\n",
    "    is_bigram = num_words == 2\n",
    "    is_trigram = num_words == 3\n",
    "\n",
    "    \n",
    "    # Remove stopwords\n",
    "    tokens = [word for word in tokens if word not in custom_stop_words and \"_\" not in word]\n",
    "    \n",
    "    # Handle bigrams/trigrams:\n",
    "    if is_bigram:\n",
    "        # If any word in the bigram is a stopword, return an empty string\n",
    "        if any(word in custom_stop_words for word in tokens):\n",
    "            return \"\"  # Remove entire bigram if it contains a stopword\n",
    "\n",
    "    elif is_trigram:\n",
    "        # If there are two or more stopwords in the trigram, return an empty string\n",
    "        if sum(word in custom_stop_words for word in tokens) >= 2:\n",
    "            return \"\"  # Remove entire trigram if it contains two or more stopwords\n",
    "\n",
    "    return \" \".join(tokens)  # Return cleaned n-gram\n",
    "\n",
    "\n",
    "# Preprocess function on a sentence level\n",
    "def preprocess(text):\n",
    "    # Split the text into sentences\n",
    "    sentences = sent_tokenize(text)\n",
    "\n",
    "    # Preprocess each sentence\n",
    "    processed_sentences = []\n",
    "    for sentence in sentences:\n",
    "        # Convert to lowercase\n",
    "        sentence = sentence.lower()\n",
    "        \n",
    "        # Remove reference section if present\n",
    "        sentence = re.sub(r\"references[\\s\\S]*\", \"\", sentence, flags=re.IGNORECASE)\n",
    "\n",
    "        # Remove mentions of email addresses\n",
    "        sentence = re.sub(r\"\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,}\\b\", \"\", sentence)\n",
    "\n",
    "        # Remove standard metadata phrases\n",
    "        metadata_patterns = [\n",
    "            r\"downloaded from.*?wiley online library(\\n|$)\",  \n",
    "            r\"see the terms and conditions.*?(\\n|$)\",\n",
    "            r\"published online by cambridge university press(\\n|$)\",\n",
    "            r\"this is an open access article.*?creative commons license(\\n|$)\",\n",
    "            r\"american journal of political science, vol\\..*?, pp\\..*?(\\n|$)\",\n",
    "            r\"copyright ©.*?(\\n|$)\",\n",
    "            r\"first published online.*?(\\n|$)\",\n",
    "            r\"earlier versions of this article were presented at.*?(\\n|$)\"\n",
    "        ]\n",
    "        for pattern in metadata_patterns:\n",
    "            sentence = re.sub(pattern, \"\", sentence, flags=re.IGNORECASE | re.MULTILINE)\n",
    "\n",
    "        # Remove URLs and numbers\n",
    "        sentence = re.sub(r\"http\\S+|www\\S+|https\\S+\", \"\", sentence)  # Remove URLs\n",
    "        sentence = re.sub(r\"\\d+\", \"\", sentence)  # Remove numbers\n",
    "\n",
    "        # Replace specific characters with spaces\n",
    "        sentence = re.sub(r\"['‘’“”]\", \" \", sentence)  # Remove quotes and apostrophes\n",
    "        sentence = re.sub(r\"[-_]\", \" \", sentence)  # Replace hyphens and underscores with spaces\n",
    "\n",
    "        # Remove punctuation except for apostrophes and hyphens\n",
    "        sentence = re.sub(r\"[^\\w\\s'-]\", \" \", sentence)  # Keep letters, numbers, spaces, apostrophes, and hyphens\n",
    "\n",
    "        # Tokenize the sentence into words\n",
    "        tokens = word_tokenize(sentence)\n",
    "\n",
    "        # Remove stopwords\n",
    "        tokens = [word for word in tokens if word not in custom_stop_words]\n",
    "\n",
    "        # Rejoin the tokens into the processed sentence\n",
    "        processed_sentence = \" \".join(tokens)\n",
    "        processed_sentences.append(processed_sentence)\n",
    "    \n",
    "    # Rejoin the processed sentences into the final preprocessed text\n",
    "    processed_text = ' SENTENCESPLITHERE '.join(processed_sentences)\n",
    "    \n",
    "    return processed_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming you have already defined preprocess function and your DataFrame `df` is ready\n",
    "output_path = '/Users/yvette/Desktop/data/Final/preprocessed grouped txt'\n",
    "\n",
    "# Apply preprocessing to the text column\n",
    "df['preprocessed_text'] = df['text'].apply(preprocess)\n",
    "\n",
    "# Group the DataFrame by 'year_range' and 'isPartOf'\n",
    "grouped_df = df.groupby(['year_range', 'isPartOf'])\n",
    "\n",
    "token_counts = []\n",
    "\n",
    "# Loop through each group and save the output to a .txt file\n",
    "for (year_range, is_part_of), group in grouped_df:\n",
    "    # Construct the filename\n",
    "    filename = f'text_{year_range}_{is_part_of}.txt'\n",
    "    file_path = os.path.join(output_path, filename)\n",
    "    \n",
    "    # Ensure the output directory exists\n",
    "    os.makedirs(output_path, exist_ok=True)\n",
    "    \n",
    "    # Combine all the preprocessed text of the group into one string\n",
    "    group_text = \"\\n\".join(group['preprocessed_text'].dropna())\n",
    "    token_count = len(group_text.split())  \n",
    "    token_counts.append({\"group_name\": filename, \"token_count\": token_count})\n",
    "    \n",
    "    \n",
    "    # Write the text to the file\n",
    "    with open(file_path, 'w', encoding='utf-8') as f:\n",
    "        f.write(group_text)\n",
    "    \n",
    "    # Print confirmation message\n",
    "    print(f\"Saved: {file_path}\")\n",
    "\n",
    "token_counts_df = pd.DataFrame(token_counts)\n",
    "print(token_counts_df)\n",
    "\n",
    "token_counts_df.to_csv(\"/Users/yvette/Desktop/group_token_counts.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to the folder containing the ngram CSV files\n",
    "folder_path = '/Users/yvette/Desktop/data/Final/ngram word frequency'\n",
    "output_folder = \"/Users/yvette/Desktop/data/Final/ngram word frequency without stopwords\"\n",
    "\n",
    "# Ensure the output directory exists\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "# Loop through each file in the folder\n",
    "for filename in os.listdir(folder_path):\n",
    "    if filename.endswith(\".csv\"):  # Process only CSV files\n",
    "        file_path = os.path.join(folder_path, filename)\n",
    "\n",
    "        # Read the CSV file into a DataFrame\n",
    "        df_ngram = pd.read_csv(file_path)\n",
    "\n",
    "        # Check if 'ngram' column exists\n",
    "        if 'ngram' in df_ngram.columns:\n",
    "            # Apply preprocessing (you should define your preprocess_ngram function beforehand)\n",
    "            df_ngram['ngram'] = df_ngram['ngram'].apply(preprocess_ngram)\n",
    "\n",
    "            # Remove rows where 'ngram' is empty\n",
    "            df_ngram = df_ngram[df_ngram['ngram'].str.strip() != \"\"]\n",
    "\n",
    "            # Remove bigrams with only one word and trigrams with two words based on filename\n",
    "            condition = (\n",
    "                (df_ngram['ngram'].apply(lambda x: len(x.split()) == 1) & filename.__contains__(\"unigram\")) |\n",
    "                (df_ngram['ngram'].apply(lambda x: len(x.split()) == 2) & filename.__contains__(\"bigram\")) |\n",
    "                (df_ngram['ngram'].apply(lambda x: len(x.split()) == 3) & filename.__contains__(\"trigram\"))\n",
    "            )\n",
    "            df_ngram = df_ngram[condition]\n",
    "\n",
    "            # Save the preprocessed DataFrame\n",
    "            new_filename = filename.replace(\".csv\", \"_without_stopword.csv\")\n",
    "            new_file_path = os.path.join(output_folder, new_filename)\n",
    "            df_ngram.to_csv(new_file_path, index=False)\n",
    "\n",
    "            print(f\"Processed and saved: {new_filename}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
