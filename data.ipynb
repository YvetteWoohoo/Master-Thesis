{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                            abstract  \\\n",
      "0  How is the unjust global order reproduced, and...   \n",
      "1                                                NaN   \n",
      "2  Work exploring the relationship between public...   \n",
      "3                                                NaN   \n",
      "4                                                NaN   \n",
      "\n",
      "                                             creator datePublished  \\\n",
      "0                                 [Anahí Wiedenbrüg]    2021-04-01   \n",
      "1                                  [R. A. W. Rhodes]    1984-07-01   \n",
      "2            [Stuart N. Soroka, Christopher Wlezien]    2005-10-01   \n",
      "3  [Alan Warde, Gindo Tampubolon, Brian Longhurst...    2003-07-01   \n",
      "4  [Richard M. Merelman, R. W. Connell, Herbert H...    1973-02-01   \n",
      "\n",
      "         docSubType  docType  \\\n",
      "0  research-article  article   \n",
      "1  research-article  article   \n",
      "2  research-article  article   \n",
      "3  research-article  article   \n",
      "4       book-review  article   \n",
      "\n",
      "                                            fullText  \\\n",
      "0  [Responsibility For Financial Crises AnahiWied...   \n",
      "1  [B.J.Pol.S. 14, 261-283 Printed in Great Brita...   \n",
      "2  [B.J.Pol.S. 35, 665-689 Copyright @ 2005 Cambr...   \n",
      "3  [B.J.Pol.S. 33, 515-534 Copyright @ 2003 Cambr...   \n",
      "4  [Book Reviews The Child's Construction of Poli...   \n",
      "\n",
      "                                     id  \\\n",
      "0  http://www.jstor.org/stable/45380815   \n",
      "1    http://www.jstor.org/stable/193954   \n",
      "2   http://www.jstor.org/stable/4092416   \n",
      "3   http://www.jstor.org/stable/4092309   \n",
      "4   http://www.jstor.org/stable/2110485   \n",
      "\n",
      "                                          identifier  \\\n",
      "0  [{'name': 'local_doi', 'value': '10.2307/45380...   \n",
      "1  [{'name': 'local_doi', 'value': '10.2307/19395...   \n",
      "2  [{'name': 'local_doi', 'value': '10.2307/40924...   \n",
      "3  [{'name': 'local_doi', 'value': '10.2307/40923...   \n",
      "4  [{'name': 'local_doi', 'value': '10.2307/21104...   \n",
      "\n",
      "                                isPartOf issueNumber  ...   pagination  \\\n",
      "0  American Journal of Political Science           2  ...  pp. 460-472   \n",
      "1   British Journal of Political Science           3  ...  pp. 261-283   \n",
      "2   British Journal of Political Science           4  ...  pp. 665-689   \n",
      "3   British Journal of Political Science           3  ...  pp. 515-525   \n",
      "4  American Journal of Political Science           1  ...  pp. 205-207   \n",
      "\n",
      "  provider publicationYear                              publisher  \\\n",
      "0    jstor            2021  Midwest Political Science Association   \n",
      "1    jstor            1984             Cambridge University Press   \n",
      "2    jstor            2005             Cambridge University Press   \n",
      "3    jstor            2003             Cambridge University Press   \n",
      "4    jstor            1973  Midwest Political Science Association   \n",
      "\n",
      "                                      sourceCategory  \\\n",
      "0  [Political Science, American Studies, Social S...   \n",
      "1               [Political Science, Social Sciences]   \n",
      "2               [Political Science, Social Sciences]   \n",
      "3               [Political Science, Social Sciences]   \n",
      "4  [Political Science, American Studies, Social S...   \n",
      "\n",
      "                                         tdmCategory  \\\n",
      "0  [Political science - Politics, Philosophy - Ap...   \n",
      "1  [Economics - Economic policy, Philosophy - App...   \n",
      "2  [Business - Business operations, Economics - E...   \n",
      "3  [Economics - Economic disciplines, Mathematics...   \n",
      "4                  [Philosophy - Applied philosophy]   \n",
      "\n",
      "                                               title  \\\n",
      "0                Responsibility For Financial Crises   \n",
      "1  Continuity and Change in British Central–Local...   \n",
      "2  Opinion-Policy Dynamics: Public Preferences an...   \n",
      "3  Trends in Social Capital: Membership of Associ...   \n",
      "4               The Child's Construction of Politics   \n",
      "\n",
      "                                    url  volumeNumber wordCount  \n",
      "0  http://www.jstor.org/stable/45380815            65     10948  \n",
      "1    http://www.jstor.org/stable/193954            14     11292  \n",
      "2   http://www.jstor.org/stable/4092416            35     12484  \n",
      "3   http://www.jstor.org/stable/4092309            33      5160  \n",
      "4   http://www.jstor.org/stable/2110485            17      1221  \n",
      "\n",
      "[5 rows x 26 columns]\n"
     ]
    }
   ],
   "source": [
    "## jsonl\n",
    "import pandas as pd\n",
    "import json\n",
    "import re\n",
    "# Path to your JSON file\n",
    "file_path = \"/Users/yvette/Desktop/data/part-1-all.jsonl\"\n",
    "\n",
    "# Read the JSONL file line by line\n",
    "with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "    data = [json.loads(line) for line in file]\n",
    "\n",
    "# Convert to DataFrame\n",
    "df = pd.json_normalize(data)\n",
    "print(df.head())\n",
    "\n",
    "titles_to_remove = [\"Back Matter\", \"Front Matter\", \"Volume Information\", \"In Forthcoming Issues\",\"Forthcoming Issues\",\"Note from the Editor\",\"Correction: The Child's Conception of the Queen and the Prime Minister\",\"Correction for: The Length of Ministerial Tenure in the United Kingdom, 1945-97\"]\n",
    "df = df[~df['title'].isin(titles_to_remove) & \n",
    "        ~df['docSubType'].isin(['misc', 'news']) &\n",
    "        ~df['title'].str.contains('Errat', case=False, na=False)]\n",
    "\n",
    "df.rename(columns={'fullText': 'text'}, inplace=True)\n",
    "df['creator'] = df['creator'].apply(lambda x: re.sub(r'\\[|\\]', '', str(x)))\n",
    "df['creator'] = df['creator'].apply(lambda x: re.sub(r\"'\", \"\", str(x)))\n",
    "df['text'] = df['text'].apply(lambda x: re.sub(r\"\\[|\\]\", \"\", str(x)))\n",
    "df['text'] = df['text'].apply(lambda x: re.sub(r'\"', '', str(x)))\n",
    "df['text'] = df['text'].apply(lambda x: re.sub(r\"'\", '', str(x)))\n",
    "df['keyphrase'] = df['keyphrase'].apply(lambda x: re.sub(r'\\[|\\]', '', str(x)))\n",
    "df['keyphrase'] = df['keyphrase'].apply(lambda x: re.sub(r\"'\", \"\", str(x)))\n",
    "df['language'] = df['language'].apply(lambda x: re.sub(r'\\[|\\]', '', str(x)))\n",
    "df['language'] = df['language'].apply(lambda x: re.sub(r\"'\", \"\", str(x)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## APS 783 pdf to txt\n",
    "import os\n",
    "import fitz  # PyMuPDF\n",
    "from tqdm import tqdm\n",
    "\n",
    "# input folder and output folder\n",
    "input_folder = \"/Users/yvette/Desktop/data/APS 783\"  \n",
    "output_folder = \"/Users/yvette/Desktop/data/APS 783 txt\"  \n",
    "\n",
    "# make sure the output folder exists\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "# get all pdf files in the input folder\n",
    "pdf_files = [f for f in os.listdir(input_folder) if f.endswith(\".pdf\")]\n",
    "\n",
    "def pdf_to_text(pdf_path):\n",
    "    text = []\n",
    "    doc = fitz.open(pdf_path)\n",
    "    for page in doc:\n",
    "        page_text = page.get_text(\"text\").strip()  \n",
    "        text.append(page_text)\n",
    "    return \"\\n\".join(text)\n",
    "\n",
    "# iterate through all pdf files and convert them to text\n",
    "for pdf_file in tqdm(pdf_files, desc=\"Converting PDFs to TXT\"):\n",
    "    pdf_path = os.path.join(input_folder, pdf_file)\n",
    "    text = pdf_to_text(pdf_path)\n",
    "    \n",
    "    # generate the output text file name\n",
    "    txt_filename = os.path.splitext(pdf_file)[0] + \".txt\"\n",
    "    txt_path = os.path.join(output_folder, txt_filename)\n",
    "    \n",
    "    # write the text to a file\n",
    "    with open(txt_path, \"w\", encoding=\"utf-8\") as txt_file:\n",
    "        txt_file.write(text)\n",
    "\n",
    "print(f\"所有 PDF 已转换为 TXT，保存在 {output_folder} 文件夹。\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## APS 783 txt combined with metadata\n",
    "import os\n",
    "import pandas as pd\n",
    "import openpyxl\n",
    "import re\n",
    "import urllib.parse\n",
    "from rapidfuzz import process, fuzz\n",
    "\n",
    "# Input and output paths\n",
    "input_folder = \"/Users/yvette/Desktop/data/APS 783 txt\"  # Your folder containing TXT files\n",
    "\n",
    "data = []\n",
    "\n",
    "# Process each TXT file\n",
    "for txt_file in os.listdir(input_folder):\n",
    "    if txt_file.endswith(\".txt\"):\n",
    "        file_path = os.path.join(input_folder, txt_file)\n",
    "        \n",
    "        # Extract the file name (remove .txt)\n",
    "        filename_without_ext = os.path.splitext(txt_file)[0]\n",
    "\n",
    "        # **Improve title extraction** (remove year and author)\n",
    "        match = re.search(r\"\\d{4} [A-Za-z]+ (.+)\", filename_without_ext)\n",
    "        if match:\n",
    "            title = match.group(1).strip()  # Only extract article title\n",
    "        else:\n",
    "            title = filename_without_ext.strip()  # Keep full file name if no match\n",
    "        \n",
    "        # Read the TXT file content\n",
    "        with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            content = f.read()  # Read the entire file, remove extra spaces\n",
    "\n",
    "        # Add the title and content to the data list\n",
    "        data.append([title, content])  # Store as a list with title and content\n",
    "\n",
    "# Convert the list of data to a DataFrame\n",
    "APS_783 = pd.DataFrame(data, columns=[\"title\", \"text\"])\n",
    "APS_783['title'] = APS_783['title'].apply(lambda x: urllib.parse.unquote(x))\n",
    "print(APS_783.head())\n",
    "\n",
    "file_path = \"/Users/yvette/Desktop/data/APS-portico-metadata.csv\"\n",
    "df_csv = pd.read_csv(file_path)\n",
    "\n",
    "# Remove unwanted rows\n",
    "df_filtered = df_csv[~df_csv['title'].isin(['Issue Information ‐ Table of Contents', 'Issue Information'])]\n",
    "\n",
    "print(df_filtered.shape)\n",
    "\n",
    "def find_best_match(title, choices):\n",
    "    match, score, index = process.extractOne(title, choices, scorer=fuzz.ratio)  # Use fuzz.ratio for string similarity\n",
    "    return match if score > 63 else None  \n",
    "\n",
    "\n",
    "df_filtered[\"matched_title\"] = df_filtered[\"title\"].apply(lambda x: find_best_match(x, APS_783[\"title\"].tolist()))\n",
    "\n",
    "print(df_filtered.head())\n",
    "print(df_filtered.shape)\n",
    "print(df_filtered[df_filtered[\"matched_title\"].isna()])\n",
    "\n",
    "# Merge the filtered DataFrame with the APS_783 DataFrame\n",
    "df_merged_APS_783 = df_filtered.merge(APS_783[[\"title\", \"text\"]], left_on=\"matched_title\", right_on=\"title\", how=\"left\")\n",
    "df_merged_APS_783 = df_merged_APS_783.drop(columns=[\"matched_title\", \"title_y\"]).rename(columns={\"title_x\": \"title\"})\n",
    "\n",
    "print(df_merged_APS_783.shape)\n",
    "print(df_merged_APS_783.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## BPS 909 pdf to txt\n",
    "import os\n",
    "import fitz  # PyMuPDF\n",
    "from tqdm import tqdm\n",
    "\n",
    "# input folder and output folder\n",
    "input_folder = \"/Users/yvette/Desktop/data/BPS 909\"  \n",
    "output_folder = \"/Users/yvette/Desktop/data/BPS 909 txt\"  \n",
    "\n",
    "# make sure the output folder exists\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "# get all pdf files in the input folder\n",
    "pdf_files = [f for f in os.listdir(input_folder) if f.endswith(\".pdf\")]\n",
    "\n",
    "def pdf_to_text(pdf_path):\n",
    "    text = []\n",
    "    doc = fitz.open(pdf_path)\n",
    "    for page in doc:\n",
    "        page_text = page.get_text(\"text\").strip()  \n",
    "        text.append(page_text)\n",
    "    return \"\\n\".join(text)\n",
    "\n",
    "# iterate through all pdf files and convert them to text\n",
    "for pdf_file in tqdm(pdf_files, desc=\"Converting PDFs to TXT\"):\n",
    "    pdf_path = os.path.join(input_folder, pdf_file)\n",
    "    text = pdf_to_text(pdf_path)\n",
    "    \n",
    "    # generate the output text file name\n",
    "    txt_filename = os.path.splitext(pdf_file)[0] + \".txt\"\n",
    "    txt_path = os.path.join(output_folder, txt_filename)\n",
    "    \n",
    "    # write the text to a file\n",
    "    with open(txt_path, \"w\", encoding=\"utf-8\") as txt_file:\n",
    "        txt_file.write(text)\n",
    "\n",
    "print(f\"所有 PDF 已转换为 TXT，保存在 {output_folder} 文件夹。\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## BPS 909 txt combined with metadata\n",
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "import openpyxl\n",
    "\n",
    "# Input and output paths\n",
    "input_folder = \"/Users/yvette/Desktop/data/BPS 909 txt\" # Your folder containing TXT files\n",
    "\n",
    "data = []\n",
    "\n",
    "# Process each TXT file\n",
    "for txt_file in os.listdir(input_folder):\n",
    "    if txt_file.endswith(\".txt\"):\n",
    "        file_path = os.path.join(input_folder, txt_file)\n",
    "        \n",
    "        # Extract the file name (remove .txt)\n",
    "        filename_without_ext = os.path.splitext(txt_file)[0]\n",
    "\n",
    "        # Set title as the filename without extension\n",
    "        title = filename_without_ext.replace(\"-\", \" \")\n",
    "        \n",
    "        # Read the TXT file content\n",
    "        with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            content = f.read()\n",
    "        # Add the title and content to the data list\n",
    "        data.append([title, content])  # Store as a list with title and content\n",
    "\n",
    "# Convert the list of data to a DataFrame\n",
    "BPS_909 = pd.DataFrame(data, columns=[\"title\", \"text\"])\n",
    "\n",
    "# Read the Excel file\n",
    "file_path = \"/Users/yvette/Desktop/data/BPS-portico-metadata.xlsx\"\n",
    "wb = openpyxl.load_workbook(file_path)\n",
    "ws = wb.active  # 选择第一个工作表\n",
    "\n",
    "# find the index of the \"title\" column\n",
    "header = [cell.value for cell in next(ws.iter_rows(min_row=1, max_row=1))]\n",
    "title_col_index = header.index(\"title\")\n",
    "\n",
    "# read the rows and filter out red font color\n",
    "rows_to_keep = []\n",
    "for row in ws.iter_rows():\n",
    "    cell = row[title_col_index] \n",
    "    if cell.font.color is None or (cell.font.color.rgb != \"FFFF0000\"):  \n",
    "        rows_to_keep.append([c.value for c in row])\n",
    "\n",
    "# Convert to DataFrame\n",
    "df_excel = pd.DataFrame(rows_to_keep[1:], columns=rows_to_keep[0])  \n",
    "df_excel[\"title\"] = df_excel[\"title\"].str.lower().str.replace(r\"[:,.!?'-()]\", \"\", regex=True)  \n",
    "\n",
    "def find_best_match(title, choices):\n",
    "    match, score, index = process.extractOne(title, choices, scorer=fuzz.ratio)  # Use fuzz.ratio for string similarity\n",
    "    return match if score > 60 else None  \n",
    "\n",
    "\n",
    "df_excel[\"matched_title\"] = df_excel[\"title\"].apply(lambda x: find_best_match(x, BPS_909[\"title\"].tolist()))\n",
    "\n",
    "print(df_excel[df_excel[\"matched_title\"].isna()])\n",
    "\n",
    "\n",
    "# Merge the filtered DataFrame with the BPS_909 DataFrame\n",
    "df_merged_BPS_909 = df_excel.merge(BPS_909[[\"title\", \"text\"]], left_on=\"matched_title\", right_on=\"title\", how=\"left\")\n",
    "df_merged_BPS_909 = df_merged_BPS_909.drop(columns=[\"matched_title\", \"title_y\"]).rename(columns={\"title_x\": \"title\"})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# APS 60 pdf to txt\n",
    "import os\n",
    "import fitz  # PyMuPDF\n",
    "from tqdm import tqdm\n",
    "\n",
    "# input folder and output folder\n",
    "input_folder = \"/Users/yvette/Desktop/data/APS all 60\"  \n",
    "output_folder = \"/Users/yvette/Desktop/data/APS all 60 txt\"  \n",
    "\n",
    "# make sure the output folder exists\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "# get all pdf files in the input folder\n",
    "pdf_files = [f for f in os.listdir(input_folder) if f.endswith(\".pdf\")]\n",
    "\n",
    "def pdf_to_text(pdf_path):\n",
    "    \"\"\"将 PDF 转换为干净的文本格式\"\"\"\n",
    "    text = []\n",
    "    doc = fitz.open(pdf_path)\n",
    "    for page in doc:\n",
    "        page_text = page.get_text(\"text\").strip()  \n",
    "        text.append(page_text)\n",
    "    return \"\\n\".join(text)\n",
    "\n",
    "# iterate through all pdf files and convert them to text\n",
    "for pdf_file in tqdm(pdf_files, desc=\"Converting PDFs to TXT\"):\n",
    "    pdf_path = os.path.join(input_folder, pdf_file)\n",
    "    text = pdf_to_text(pdf_path)\n",
    "    \n",
    "    # generate the output text file name\n",
    "    txt_filename = os.path.splitext(pdf_file)[0] + \".txt\"\n",
    "    txt_path = os.path.join(output_folder, txt_filename)\n",
    "    \n",
    "    # write the text to a file\n",
    "    with open(txt_path, \"w\", encoding=\"utf-8\") as txt_file:\n",
    "        txt_file.write(text)\n",
    "\n",
    "print(f\"所有 PDF 已转换为 TXT，保存在 {output_folder} 文件夹。\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Converting PDFs to TXT: 100%|██████████| 47/47 [00:02<00:00, 18.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "所有 PDF 已转换为 TXT，保存在 /Users/yvette/Desktop/data/BPS all 47 txt 文件夹。\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# BPS 47 pdf to txt\n",
    "# input folder and output folder\n",
    "input_folder = \"/Users/yvette/Desktop/data/BPS all 47\"  \n",
    "output_folder = \"/Users/yvette/Desktop/data/BPS all 47 txt\"  \n",
    "\n",
    "# make sure the output folder exists\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "# get all pdf files in the input folder\n",
    "pdf_files = [f for f in os.listdir(input_folder) if f.endswith(\".pdf\")]\n",
    "\n",
    "def pdf_to_text(pdf_path):\n",
    "    \"\"\"将 PDF 转换为干净的文本格式\"\"\"\n",
    "    text = []\n",
    "    doc = fitz.open(pdf_path)\n",
    "    for page in doc:\n",
    "        page_text = page.get_text(\"text\").strip() \n",
    "        text.append(page_text)\n",
    "    return \"\\n\".join(text)\n",
    "\n",
    "# iterate through all pdf files and convert them to text\n",
    "for pdf_file in tqdm(pdf_files, desc=\"Converting PDFs to TXT\"):\n",
    "    pdf_path = os.path.join(input_folder, pdf_file)\n",
    "    text = pdf_to_text(pdf_path)\n",
    "    \n",
    "    # generate the output text file name\n",
    "    txt_filename = os.path.splitext(pdf_file)[0] + \".txt\"\n",
    "    txt_path = os.path.join(output_folder, txt_filename)\n",
    "    \n",
    "    # write the text to a file\n",
    "    with open(txt_path, \"w\", encoding=\"utf-8\") as txt_file:\n",
    "        txt_file.write(text)\n",
    "\n",
    "print(f\"所有 PDF 已转换为 TXT，保存在 {output_folder} 文件夹。\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                               title  \\\n",
      "0  Aid  Attitudes  and Insurgency  Evidence from ...   \n",
      "1  When Should We Use Unit Fixed Effects Regressi...   \n",
      "2   Legislative Capacity and Executive Unilateralism   \n",
      "3  The Problem of Political Science  Political Re...   \n",
      "4              When Governments Regulate Governments   \n",
      "\n",
      "                                                text  \n",
      "0  Aid, Attitudes, and Insurgency: Evidence from\\...  \n",
      "1  When Should We Use Unit Fixed Effects Regressi...  \n",
      "2  Legislative Capacity and Executive Unilaterali...  \n",
      "3  The Problem of Political Science: Political Re...  \n",
      "4  When Governments Regulate Governments\\nDavid M...  \n",
      "Empty DataFrame\n",
      "Columns: [id, title, isPartOf, publicationYear, doi, docType, docSubType, provider, collection, datePublished, issueNumber, volumeNumber, url, creator, publisher, language, pageStart, pageEnd, placeOfPublication, keyphrase, wordCount, pageCount, outputFormat, matched_title]\n",
      "Index: []\n",
      "\n",
      "[0 rows x 24 columns]\n"
     ]
    }
   ],
   "source": [
    "# APS 60, BPS 47 merge with metadata\n",
    "import os\n",
    "import pandas as pd\n",
    "import openpyxl\n",
    "import re\n",
    "import urllib.parse\n",
    "from rapidfuzz import process, fuzz\n",
    "\n",
    "# Input and output paths\n",
    "input_folder = \"/Users/yvette/Desktop/data/APS all 60 txt\"  # Your folder containing TXT files\n",
    "\n",
    "data = []\n",
    "\n",
    "# Process each TXT file\n",
    "for txt_file in os.listdir(input_folder):\n",
    "    if txt_file.endswith(\".txt\"):\n",
    "        file_path = os.path.join(input_folder, txt_file)\n",
    "        \n",
    "        # Extract the file name (remove .txt)\n",
    "        filename_without_ext = os.path.splitext(txt_file)[0]\n",
    "\n",
    "        # **Improve title extraction** (remove year and author)\n",
    "        match = re.search(r\"\\d{4} [A-Za-z]+ (.+)\", filename_without_ext)\n",
    "        if match:\n",
    "            title = match.group(1).strip()  # Only extract article title\n",
    "        else:\n",
    "            title = filename_without_ext.strip()  # Keep full file name if no match\n",
    "        \n",
    "        # Read the TXT file content\n",
    "        with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            content = f.read()  # Read the entire file, remove extra spaces\n",
    "\n",
    "        # Add the title and content to the data list\n",
    "        data.append([title, content])  # Store as a list with title and content\n",
    "\n",
    "# Convert the list of data to a DataFrame\n",
    "APS_60 = pd.DataFrame(data, columns=[\"title\", \"text\"])\n",
    "APS_60['title'] = APS_60['title'].apply(lambda x: urllib.parse.unquote(x))\n",
    "print(APS_60.head())\n",
    "\n",
    "# Input and output paths\n",
    "input_folder = \"/Users/yvette/Desktop/data/BPS all 47 txt\" # Your folder containing TXT files\n",
    "\n",
    "data = []\n",
    "# Process each TXT file\n",
    "for txt_file in os.listdir(input_folder):\n",
    "    if txt_file.endswith(\".txt\"):\n",
    "        file_path = os.path.join(input_folder, txt_file)\n",
    "        \n",
    "        # Extract the file name (remove .txt)\n",
    "        filename_without_ext = os.path.splitext(txt_file)[0]\n",
    "\n",
    "        # **Improve title extraction** (remove year and author)\n",
    "        match = re.search(r\"\\d{4} [A-Za-z]+ (.+)\", filename_without_ext)\n",
    "        if match:\n",
    "            title = match.group(1).strip()  # Only extract article title\n",
    "        else:\n",
    "            title = filename_without_ext.strip()  # Keep full file name if no match\n",
    "        \n",
    "        # Read the TXT file content\n",
    "        with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            content = f.read()  # Read the entire file, remove extra spaces\n",
    "\n",
    "        # Add the title and content to the data list\n",
    "        data.append([title, content])  # Store as a list with title and content\n",
    "# Convert the list of data to a DataFrame\n",
    "BPS_47 = pd.DataFrame(data, columns=[\"title\", \"text\"])\n",
    "\n",
    "merged_60_47 = pd.concat([APS_60, BPS_47], ignore_index=True)\n",
    "\n",
    "# Read the Excel file\n",
    "file_path = \"/Users/yvette/Desktop/data/filtered-all-portico-metadata.xlsx\"\n",
    "wb = openpyxl.load_workbook(file_path)\n",
    "ws = wb.active  \n",
    "\n",
    "# find the index of the \"title\" column\n",
    "header = [cell.value for cell in next(ws.iter_rows(min_row=1, max_row=1))]\n",
    "title_col_index = header.index(\"title\")\n",
    "\n",
    "# read the rows and filter out red font color\n",
    "rows_to_keep = []\n",
    "for row in ws.iter_rows():\n",
    "    cell = row[title_col_index]  \n",
    "    if cell.font.color is None or (cell.font.color.rgb != \"FFFF0000\"):  \n",
    "        rows_to_keep.append([c.value for c in row])\n",
    "\n",
    "# Convert to DataFrame\n",
    "df_excel = pd.DataFrame(rows_to_keep[1:], columns=rows_to_keep[0])  \n",
    "df_excel[\"title\"] = df_excel[\"title\"].str.lower().str.replace(r\"[:,.!?'-()]\", \"\", regex=True)  \n",
    "\n",
    "def find_best_match(title, choices):\n",
    "    match, score, index = process.extractOne(title, choices, scorer=fuzz.ratio)  # Use fuzz.ratio for string similarity\n",
    "    return match if score > 60 else None  \n",
    "\n",
    "df_excel[\"matched_title\"] = df_excel[\"title\"].apply(lambda x: find_best_match(x, merged_60_47[\"title\"].tolist()))\n",
    "\n",
    "print(df_excel[df_excel[\"matched_title\"].isna()])\n",
    "\n",
    "\n",
    "# Merge the filtered DataFrame with the merged_60_47 DataFrame\n",
    "df_merged_60_47 = df_excel.merge(merged_60_47[[\"title\", \"text\"]], left_on=\"matched_title\", right_on=\"title\", how=\"left\")\n",
    "df_merged_60_47 = df_merged_60_47.drop(columns=[\"matched_title\", \"title_y\"]).rename(columns={\"title_x\": \"title\"})\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['id', 'title', 'isPartOf', 'publicationYear', 'doi', 'docType',\n",
      "       'docSubType', 'provider', 'collection', 'datePublished', 'issueNumber',\n",
      "       'volumeNumber', 'url', 'creator', 'publisher', 'language', 'pageStart',\n",
      "       'pageEnd', 'placeOfPublication', 'keyphrase', 'wordCount', 'pageCount',\n",
      "       'outputFormat', 'text'],\n",
      "      dtype='object')\n",
      "Index(['id', 'title', 'isPartOf', 'publicationYear', 'doi', 'docType',\n",
      "       'docSubType', 'provider', 'collection', 'datePublished', 'issueNumber',\n",
      "       'volumeNumber', 'url', 'creator', 'publisher', 'language', 'pageStart',\n",
      "       'pageEnd', 'placeOfPublication', 'keyphrase', 'wordCount', 'pageCount',\n",
      "       'outputFormat', 'text'],\n",
      "      dtype='object')\n",
      "Index(['id', 'title', 'isPartOf', 'publicationYear', 'doi', 'docType',\n",
      "       'docSubType', 'provider', 'collection', 'datePublished', 'issueNumber',\n",
      "       'volumeNumber', 'url', 'creator', 'publisher', 'language', 'pageStart',\n",
      "       'pageEnd', 'placeOfPublication', 'keyphrase', 'wordCount', 'pageCount',\n",
      "       'outputFormat', 'text'],\n",
      "      dtype='object')\n",
      "Index(['abstract', 'creator', 'datePublished', 'docSubType', 'docType', 'text',\n",
      "       'id', 'identifier', 'isPartOf', 'issueNumber', 'keyphrase', 'language',\n",
      "       'outputFormat', 'pageCount', 'pageEnd', 'pageStart', 'pagination',\n",
      "       'provider', 'publicationYear', 'publisher', 'sourceCategory',\n",
      "       'tdmCategory', 'title', 'url', 'volumeNumber', 'wordCount'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "## ALL together\n",
    "print(df_merged_APS_783.columns)\n",
    "print(df_merged_BPS_909.columns)\n",
    "print(df_merged_60_47.columns)    \n",
    "print(df.columns)\n",
    "# Remove specific columns from df_merged_APS_783, df_merged_BPS_909, df\n",
    "df_merged_APS_783 = df_merged_APS_783.drop(columns=['doi', 'collection', 'placeOfPublication'])\n",
    "df_merged_BPS_909 = df_merged_BPS_909.drop(columns=['doi', 'collection', 'placeOfPublication'])\n",
    "df_merged_60_47 = df_merged_60_47.drop(columns=['doi', 'collection', 'placeOfPublication'])\n",
    "df = df.drop(columns=['abstract', 'identifier', 'pagination', 'sourceCategory', 'tdmCategory'])\n",
    "df_all = pd.concat([df_merged_APS_783, df_merged_BPS_909,df_merged_60_47, df], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace \"Midwest Journal of Political Science\" with \"American Journal of Political Science\"\n",
    "df_all['isPartOf'] = df_all['isPartOf'].replace('Midwest Journal of Political Science', 'American Journal of Political Science')\n",
    "\n",
    "# Define the year ranges\n",
    "year_ranges = ['1971-1980', '1981-1990', '1991-2000', '2001-2010', '2011-2020', '2021-2024']\n",
    "\n",
    "# Create a function to categorize the year into the specified ranges\n",
    "def categorize_year(year):\n",
    "    if 1971 <= year <= 1980:\n",
    "        return '1971-1980'\n",
    "    elif 1981 <= year <= 1990:\n",
    "        return '1981-1990'\n",
    "    elif 1991 <= year <= 2000:\n",
    "        return '1991-2000'\n",
    "    elif 2001 <= year <= 2010:\n",
    "        return '2001-2010'\n",
    "    elif 2011 <= year <= 2020:\n",
    "        return '2011-2020'\n",
    "    elif 2021 <= year <= 2024:\n",
    "        return '2021-2024'\n",
    "    else:\n",
    "        return None  # For any other years outside this range\n",
    "\n",
    "# Apply the function to create a new column 'year_range'\n",
    "df_all['year_range'] = df_all['publicationYear'].apply(categorize_year)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        year  APS_articles  BPS_articles  APS_wordcount  BPS_wordcount\n",
      "0  1971-1980           472           325        2919923        2559913\n",
      "1  1981-1990           435           264        3977142        2490315\n",
      "2  1991-2000           538           279        5114556        2828226\n",
      "3  2001-2010           593           529        6944718        5812471\n",
      "4  2011-2020          1242           823       14231219        9934559\n",
      "5  2021-2024           360           314        4419635        3383388\n"
     ]
    }
   ],
   "source": [
    "# Aggregate both article counts and word count sums\n",
    "result = df_all.groupby([\"year_range\", \"isPartOf\"]).agg(\n",
    "    article_count=(\"isPartOf\", \"count\"), word_count_total=(\"wordCount\", \"sum\")\n",
    ").unstack(fill_value=0)\n",
    "\n",
    "# Flatten column names\n",
    "result.columns = [\"APS_articles\", \"BPS_articles\", \"APS_wordcount\", \"BPS_wordcount\"]\n",
    "\n",
    "# Reset index to include year column\n",
    "result = result.reset_index().rename(columns={\"year_range\": \"year\"})\n",
    "\n",
    "# Print result\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all.to_json('df_decade.jsonl', orient='records', lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                             creator datePublished  \\\n",
      "0  [Colin Rallings, Michael Laver, Michael Thrasher]    1987-10-01   \n",
      "1           [Anthony J. McGann, Eliora Van der Hout]    2009-05-15   \n",
      "2                             [Johannes T. Pedersen]    1978-02-01   \n",
      "3                     [M. Martin Boyer, André Blais]    1996-04-01   \n",
      "4                               [Rikhil R. Bhavnani]    2015-11-23   \n",
      "\n",
      "           docSubType  docType                        doi  \\\n",
      "0  article-commentary  article  10.1017/S0007123400004890   \n",
      "1    research-article  article  10.1017/S0007123409000684   \n",
      "2    research-article  article                        NaN   \n",
      "3    research-article  article  10.1017/S0007123400000405   \n",
      "4    research-article  article  10.1017/S0007123415000587   \n",
      "\n",
      "                                    id  \\\n",
      "0              ark://27927/pgh1fsbczbw   \n",
      "1               ark://27927/pf1z0g3np0   \n",
      "2  http://www.jstor.org/stable/2110667   \n",
      "3              ark://27927/pgh1fsc4nsg   \n",
      "4               ark://27927/phwjwkc9w4   \n",
      "\n",
      "                                          identifier  \\\n",
      "0  [{'name': 'local_publisher_id', 'value': 'S000...   \n",
      "1  [{'name': 'local_publisher_id', 'value': 'S000...   \n",
      "2  [{'name': 'local_doi', 'value': '10.2307/21106...   \n",
      "3  [{'name': 'local_publisher_id', 'value': 'S000...   \n",
      "4  [{'name': 'local_publisher_id', 'value': 'S000...   \n",
      "\n",
      "                                isPartOf  issueNumber  \\\n",
      "0   British Journal of Political Science            4   \n",
      "1   British Journal of Political Science            4   \n",
      "2  American Journal of Political Science            1   \n",
      "3   British Journal of Political Science            2   \n",
      "4   British Journal of Political Science            1   \n",
      "\n",
      "                                           keyphrase  ...  \\\n",
      "0  [chairs, coalition, hung councils, chairs dep,...  ...   \n",
      "1  [liberal equality, electoral, voters, fairness...  ...   \n",
      "2  [apathetic voters, partisan, elections, partis...  ...   \n",
      "3  [debates, non watchers, televised debates, imp...  ...   \n",
      "4  [malapportionment, constituencies, cabinet inc...  ...   \n",
      "\n",
      "                                         tdmCategory  \\\n",
      "0                   [Political science - Government]   \n",
      "1  [Political science - Government, Political sci...   \n",
      "2                  [Philosophy - Applied philosophy]   \n",
      "3                [Mathematics - Applied mathematics]   \n",
      "4  [Political science - Government, Political sci...   \n",
      "\n",
      "                                               title  \\\n",
      "0  Coalition Theory and Local Government: Coaliti...   \n",
      "1  Proportional Representation Within the Limits ...   \n",
      "2  Political Involvement and Partisan Change in P...   \n",
      "3  Assessing the Impact of Televised Debates: The...   \n",
      "4  The Effects of Malapportionment on Cabinet Inc...   \n",
      "\n",
      "                                        url volumeNumber wordCount  \\\n",
      "0  http://doi.org/10.1017/S0007123400004890           17      4820   \n",
      "1  http://doi.org/10.1017/S0007123409000684           39     11736   \n",
      "2       http://www.jstor.org/stable/2110667           22      4727   \n",
      "3  http://doi.org/10.1017/S0007123400000405           26     10509   \n",
      "4  http://doi.org/10.1017/S0007123415000587           48     11370   \n",
      "\n",
      "                                        unigramCount  \\\n",
      "0  {'who,': 1, 'apart': 1, 'specified': 1, 'throw...   \n",
      "1  {'May’s': 3, 'Many.5': 1, 'communication)': 1,...   \n",
      "2  {'two-party': 1, 'informa-': 1, 'these': 5, 's...   \n",
      "3  {'later': 1, '(Table': 1, 'substantive': 2, 'l...   \n",
      "4  {'2003.': 8, 'New': 10, '1989–98,': 1, 'rule':...   \n",
      "\n",
      "                                         bigramCount  \\\n",
      "0  {'to look': 2, '8 8': 1, 'theorists. One': 1, ...   \n",
      "1  {'guaranteeing that': 1, 'original situation.)...   \n",
      "2  {'(2650) Net': 1, '\"the least': 1, 'the looks'...   \n",
      "3  {'26 and': 1, 'David Sanders,': 1, 'ran LOGIT'...   \n",
      "4  {'eight times': 1, 'development.45 This': 1, '...   \n",
      "\n",
      "                                        trigramCount  \\\n",
      "0  {'the hung council': 1, 'that the possession':...   \n",
      "1  {'a majority-rule vote': 1, 'axioms. When cons...   \n",
      "2  {'the very interested': 1, 'system, the volati...   \n",
      "3  {'the 'reactions' design.': 1, 'certain number...   \n",
      "4  {'variable, the dummy': 1, '1994; Panandiker a...   \n",
      "\n",
      "                                            abstract  \\\n",
      "0                                                NaN   \n",
      "1                                                NaN   \n",
      "2  Using evidence from the S.R.C. surveys about v...   \n",
      "3                                                NaN   \n",
      "4                                                NaN   \n",
      "\n",
      "                                      sourceCategory  \n",
      "0                                                NaN  \n",
      "1                                                NaN  \n",
      "2  [Political Science, American Studies, Social S...  \n",
      "3                                                NaN  \n",
      "4                                                NaN  \n",
      "\n",
      "[5 rows x 30 columns]\n"
     ]
    }
   ],
   "source": [
    "### ngram jsonl 8144\n",
    "import dask.dataframe as dd\n",
    "\n",
    "file_path = \"/Users/yvette/Desktop/data/8144 ngram.jsonl\"\n",
    "\n",
    "ngram = dd.read_json(file_path, lines=True)\n",
    "\n",
    "print(ngram.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                    id  \\\n",
      "1               ark://27927/pf1z0g3np0   \n",
      "2  http://www.jstor.org/stable/2110667   \n",
      "4               ark://27927/phwjwkc9w4   \n",
      "5               ark://27927/pghjm06ksf   \n",
      "6              ark://27927/phz45krzm5c   \n",
      "\n",
      "                                        unigramCount  \\\n",
      "1  {'May’s': 3, 'Many.5': 1, 'communication)': 1,...   \n",
      "2  {'two-party': 1, 'informa-': 1, 'these': 5, 's...   \n",
      "4  {'2003.': 8, 'New': 10, '1989–98,': 1, 'rule':...   \n",
      "5  {'M.': 11, 'modelling.': 3, 'others': 1, 'expl...   \n",
      "6  {'(2004,': 1, 'disgusted': 2, 'long': 2, 'phil...   \n",
      "\n",
      "                                         bigramCount  \\\n",
      "1  {'guaranteeing that': 1, 'original situation.)...   \n",
      "2  {'(2650) Net': 1, '\"the least': 1, 'the looks'...   \n",
      "4  {'eight times': 1, 'development.45 This': 1, '...   \n",
      "5  {'represents the': 1, 'Everyday 55': 1, '349–6...   \n",
      "6  {'even publicly,': 1, 'contests a': 1, 'will “...   \n",
      "\n",
      "                                        trigramCount  \n",
      "1  {'a majority-rule vote': 1, 'axioms. When cons...  \n",
      "2  {'the very interested': 1, 'system, the volati...  \n",
      "4  {'variable, the dummy': 1, '1994; Panandiker a...  \n",
      "5  {'in empirical research.': 1, 'cultural (educa...  \n",
      "6  {'“Aristotle’s Great-Souled Man.”': 1, 'like t...  \n",
      "                        id                                              title  \\\n",
      "0   ark://27927/pjcswvb5kf  Buttery Guns and Welfare Hawks: The Politics o...   \n",
      "1  ark://27927/pjb7q8vz8dg  Do Campaign Contribution Limits Curb the Influ...   \n",
      "2  ark://27927/phz45krzm5c  When Toleration Becomes a Vice: Naming Aristot...   \n",
      "3  ark://27927/phzptbf6jw3  How Does Minority Political Representation Aff...   \n",
      "4  ark://27927/pgh2cghxgbg  Assortative Mating on Ideology Could Operate T...   \n",
      "\n",
      "                                isPartOf  publicationYear  docType docSubType  \\\n",
      "0  American Journal of Political Science             2011  article    article   \n",
      "1  American Journal of Political Science             2022  article    article   \n",
      "2  American Journal of Political Science             2018  article    article   \n",
      "3  American Journal of Political Science             2021  article        NaN   \n",
      "4  American Journal of Political Science             2014  article    article   \n",
      "\n",
      "  provider datePublished issueNumber volumeNumber  ... pageEnd  \\\n",
      "0  portico      2011/1/1           1           55  ...     134   \n",
      "1  portico     2022/10/1           4           66  ...     946   \n",
      "2  portico     2018/10/1           4           62  ...     860   \n",
      "3  portico      2021/7/1           3           65  ...     716   \n",
      "4  portico     2014/10/1           4           58  ...    1005   \n",
      "\n",
      "                                           keyphrase wordCount pageCount  \\\n",
      "0  military spending; conflict involvement; gover...     13004        18   \n",
      "1  donors; looser limits; contracts; campaign; co...     13446        15   \n",
      "2  toleration; aristotle; virtue; nicomachean eth...      9893        12   \n",
      "3  minority; candidate; school; student achieveme...     11242        18   \n",
      "4  ideology; assortative mating; evaluator; dusti...      8150         9   \n",
      "\n",
      "               outputFormat  \\\n",
      "0  unigram; bigram; trigram   \n",
      "1  unigram; bigram; trigram   \n",
      "2  unigram; bigram; trigram   \n",
      "3  unigram; bigram; trigram   \n",
      "4  unigram; bigram; trigram   \n",
      "\n",
      "                                                text year_range  \\\n",
      "0  Buttery Guns and Welfare Hawks: The Politics\\n...  2011-2020   \n",
      "1  Do Campaign Contribution Limits Curb\\nthe Inﬂu...  2021-2024   \n",
      "2  When Toleration Becomes a Vice: Naming Aristot...  2011-2020   \n",
      "3  How Does Minority Political Representation Aff...  2021-2024   \n",
      "4  Assortative Mating on Ideology Could Operate\\n...  2011-2020   \n",
      "\n",
      "                                        unigramCount  \\\n",
      "0  {'Building,': 1, '0.004]': 2, 'noting': 2, 'sp...   \n",
      "1  {'proclivity': 1, 'Renovating': 1, 'where': 10...   \n",
      "2  {'(2004,': 1, 'disgusted': 2, 'long': 2, 'phil...   \n",
      "3  {'State': 7, 'endorsements': 1, 'Achievement':...   \n",
      "4  {'Selection:': 1, 'Pond': 1, 'include': 3, 'Si...   \n",
      "\n",
      "                                         bigramCount  \\\n",
      "0  {'São Paulo,': 1, 'Government G': 1, 'Control...   \n",
      "1  {'Access to': 2, 'as obstacles': 1, 'Ferraz, F...   \n",
      "2  {'even publicly,': 1, 'contests a': 1, 'will “...   \n",
      "3  {'board. In': 1, 'by locally': 1, 'them due': ...   \n",
      "4  {'Social Attitudes,': 1, '62(3): 713–15.': 1, ...   \n",
      "\n",
      "                                        trigramCount  \n",
      "0  {'in shaping military': 3, 'in disguise, while...  \n",
      "1  {'by the winner’s': 1, 'of 25,000 registered':...  \n",
      "2  {'“Aristotle’s Great-Souled Man.”': 1, 'like t...  \n",
      "3  {'boards over the': 1, 'scores from 2003': 1, ...  \n",
      "4  {'25(6): 729–37. Singh,': 1, 'in the hippocamp...  \n",
      "\n",
      "[5 rows x 25 columns]\n"
     ]
    }
   ],
   "source": [
    "# get the 'id' column from df_all\n",
    "titles_in_df_all = df_all['id']\n",
    "\n",
    "# Filter ngram DataFrame to only include rows where 'id' is in titles_in_df_all\n",
    "ngram_filtered = ngram[ngram['id'].isin(titles_in_df_all)]\n",
    "\n",
    "# Count the number of unigrams, bigrams, and trigrams\n",
    "ngram_filtered = ngram_filtered[['id', 'unigramCount', 'bigramCount', 'trigramCount']]\n",
    "\n",
    "# Group by 'id' and sum the counts\n",
    "print(ngram_filtered.head())\n",
    "\n",
    "# Convert ngram_filtered (Dask DataFrame) to pandas DataFrame\n",
    "ngram_filtered_pandas = ngram_filtered.compute()\n",
    "\n",
    "# Perform the merge with df_all (which is pandas)\n",
    "df_all_merged = pd.merge(df_all, ngram_filtered_pandas[['id', 'unigramCount', 'bigramCount', 'trigramCount']], on='id', how='left')\n",
    "\n",
    "# Display the merged DataFrame\n",
    "print(df_all_merged.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all_merged.to_json('df_decade_ngram.jsonl', orient='records', lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### word frequency in subcorpus by ngram\n",
    "def clean_word(word):\n",
    "    word = word.lower()  # lowercase\n",
    "    word = re.sub(r'[\\d]', '', word)  # remove digits\n",
    "    word = re.sub(r'[^\\w\\s]', '', word)  # remove punctuation\n",
    "    word = word.strip() \n",
    "    return word if len(word) > 1 else \"\"\n",
    "\n",
    "# Generalized function to expand n-gram frequency\n",
    "def expand_ngrams(df, ngram_type):\n",
    "    rows = []\n",
    "    \n",
    "    # Determine the required n-gram length for filtering\n",
    "    ngram_length = 1  # Default to unigram if no valid ngram type is found\n",
    "    if ngram_type == 'bigramCount':\n",
    "        ngram_length = 2\n",
    "    elif ngram_type == 'trigramCount':\n",
    "        ngram_length = 3\n",
    "\n",
    "    for _, row in df.iterrows():\n",
    "        year_range = row['year_range']\n",
    "        publication = row['isPartOf']\n",
    "        ngram_dict = row.get(ngram_type, {})  # Get the unigram, bigram, or trigram\n",
    "\n",
    "        if isinstance(ngram_dict, dict):  \n",
    "            for phrase, count in ngram_dict.items():\n",
    "                # Clean each word in the n-gram (unigram, bigram, trigram)\n",
    "                cleaned_phrase = \" \".join([clean_word(w) for w in phrase.split()])\n",
    "\n",
    "                # Filter out phrases that don't meet the expected length after cleaning\n",
    "                if cleaned_phrase.strip() and len(cleaned_phrase.split()) == ngram_length:  # Ensure exact n-gram length\n",
    "                    rows.append((year_range, publication, cleaned_phrase, count))\n",
    "    \n",
    "    return pd.DataFrame(rows, columns=['year_range', 'publication', 'ngram', 'ngram_count'])\n",
    "\n",
    "# Example usage:\n",
    "unigram_df = expand_ngrams(df_all_merged, 'unigramCount')\n",
    "bigram_df = expand_ngrams(df_all_merged, 'bigramCount')\n",
    "trigram_df = expand_ngrams(df_all_merged, 'trigramCount')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: unigram_1971-1980_American Journal of Political Science.csv\n",
      "Saved: unigram_1971-1980_British Journal of Political Science.csv\n",
      "Saved: unigram_1981-1990_American Journal of Political Science.csv\n",
      "Saved: unigram_1981-1990_British Journal of Political Science.csv\n",
      "Saved: unigram_1991-2000_American Journal of Political Science.csv\n",
      "Saved: unigram_1991-2000_British Journal of Political Science.csv\n",
      "Saved: unigram_2001-2010_American Journal of Political Science.csv\n",
      "Saved: unigram_2001-2010_British Journal of Political Science.csv\n",
      "Saved: unigram_2011-2020_American Journal of Political Science.csv\n",
      "Saved: unigram_2011-2020_British Journal of Political Science.csv\n",
      "Saved: unigram_2021-2024_American Journal of Political Science.csv\n",
      "Saved: unigram_2021-2024_British Journal of Political Science.csv\n",
      "Saved: bigram_1971-1980_American Journal of Political Science.csv\n",
      "Saved: bigram_1971-1980_British Journal of Political Science.csv\n",
      "Saved: bigram_1981-1990_American Journal of Political Science.csv\n",
      "Saved: bigram_1981-1990_British Journal of Political Science.csv\n",
      "Saved: bigram_1991-2000_American Journal of Political Science.csv\n",
      "Saved: bigram_1991-2000_British Journal of Political Science.csv\n",
      "Saved: bigram_2001-2010_American Journal of Political Science.csv\n",
      "Saved: bigram_2001-2010_British Journal of Political Science.csv\n",
      "Saved: bigram_2011-2020_American Journal of Political Science.csv\n",
      "Saved: bigram_2011-2020_British Journal of Political Science.csv\n",
      "Saved: bigram_2021-2024_American Journal of Political Science.csv\n",
      "Saved: bigram_2021-2024_British Journal of Political Science.csv\n",
      "Saved: trigram_1971-1980_American Journal of Political Science.csv\n",
      "Saved: trigram_1971-1980_British Journal of Political Science.csv\n",
      "Saved: trigram_1981-1990_American Journal of Political Science.csv\n",
      "Saved: trigram_1981-1990_British Journal of Political Science.csv\n",
      "Saved: trigram_1991-2000_American Journal of Political Science.csv\n",
      "Saved: trigram_1991-2000_British Journal of Political Science.csv\n",
      "Saved: trigram_2001-2010_American Journal of Political Science.csv\n",
      "Saved: trigram_2001-2010_British Journal of Political Science.csv\n",
      "Saved: trigram_2011-2020_American Journal of Political Science.csv\n",
      "Saved: trigram_2011-2020_British Journal of Political Science.csv\n",
      "Saved: trigram_2021-2024_American Journal of Political Science.csv\n",
      "Saved: trigram_2021-2024_British Journal of Political Science.csv\n"
     ]
    }
   ],
   "source": [
    "# Function to process and save n-gram data\n",
    "def process_and_save(df, ngram_type):\n",
    "    # word frequency in subcorpus\n",
    "    subcorpus_groups = df.groupby(['year_range', 'publication', 'ngram'], as_index=False).sum()\n",
    "\n",
    "    # save to CSV\n",
    "    for (year_range, publication), group in subcorpus_groups.groupby(['year_range', 'publication']):\n",
    "        output_df = group[['ngram', 'ngram_count']].sort_values(by='ngram_count', ascending=False)\n",
    "        filename = f\"{ngram_type}_{year_range}_{publication}.csv\"\n",
    "        output_df.to_csv(filename, index=False)\n",
    "        print(f\"Saved: {filename}\")\n",
    "\n",
    "process_and_save(unigram_df, \"unigram\")\n",
    "process_and_save(bigram_df, \"bigram\")\n",
    "process_and_save(trigram_df, \"trigram\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
