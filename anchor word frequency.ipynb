{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "\n",
    "def compute_word_frequencies_from_texts(word_list, subcorpus_folder, token_counts_df):\n",
    "    \"\"\"\n",
    "    Computes word frequencies for a given list of words/phrases in each subcorpus text file.\n",
    "\n",
    "    Parameters:\n",
    "    - word_list: List of words/phrases to analyze.\n",
    "    - subcorpus_folder: Path to the folder containing 12 subcorpus text files.\n",
    "    - token_counts_df: DataFrame with columns ['subcorpus', 'total_tokens'].\n",
    "\n",
    "    Returns:\n",
    "    - A DataFrame with columns: ['word', 'subcorpus', 'count', 'normalized_frequency']\n",
    "    \"\"\"\n",
    "    results = []\n",
    "\n",
    "    # Loop through each text file in the folder\n",
    "    for filename in os.listdir(subcorpus_folder):\n",
    "        if filename.endswith(\".txt\"):\n",
    "            subcorpus_name = filename.replace(\"text_\", \"\").replace(\".txt\", \"\").strip()\n",
    "          # Extract subcorpus name\n",
    "            file_path = os.path.join(subcorpus_folder, filename)\n",
    "\n",
    "            # Read the text content\n",
    "            with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "                text = file.read()\n",
    "\n",
    "            # Count occurrences of each word/phrase\n",
    "            word_counts = Counter()\n",
    "            for word in word_list:\n",
    "                word_counts[word] = text.count(word.lower())  # Count occurrences\n",
    "\n",
    "            # Get total tokens for normalization\n",
    "            total_tokens = token_counts_df.loc[token_counts_df[\"subcorpus\"] == subcorpus_name, \"token_count\"]\n",
    "            total_tokens = total_tokens.iloc[0] if not total_tokens.empty else 1  # Avoid division by zero\n",
    "\n",
    "            # Store results\n",
    "            for word, count in word_counts.items():\n",
    "                results.append({\n",
    "                    \"word\": word,\n",
    "                    \"subcorpus\": subcorpus_name,\n",
    "                    \"count\": count,\n",
    "                    \"normalized_frequency\": count / total_tokens\n",
    "                })\n",
    "\n",
    "    # Convert results to DataFrame\n",
    "    result_df = pd.DataFrame(results)\n",
    "    \n",
    "    return result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                       word                                        subcorpus  \\\n",
      "0                 Democracy  2011-2020_American Journal of Political Science   \n",
      "1              Dictatorship  2011-2020_American Journal of Political Science   \n",
      "2                 Polyarchy  2011-2020_American Journal of Political Science   \n",
      "3                 Autocracy  2011-2020_American Journal of Political Science   \n",
      "4                 Pluralism  2011-2020_American Journal of Political Science   \n",
      "...                     ...                                              ...   \n",
      "1027  Racial discrimination  1971-1980_American Journal of Political Science   \n",
      "1028        Gender equality  1971-1980_American Journal of Political Science   \n",
      "1029         Discrimination  1971-1980_American Journal of Political Science   \n",
      "1030         Redistribution  1971-1980_American Journal of Political Science   \n",
      "1031      Income inequality  1971-1980_American Journal of Political Science   \n",
      "\n",
      "      count  normalized_frequency  \n",
      "0      7734              0.000873  \n",
      "1       503              0.000057  \n",
      "2        37              0.000004  \n",
      "3       247              0.000028  \n",
      "4       121              0.000014  \n",
      "...     ...                   ...  \n",
      "1027     11              0.000006  \n",
      "1028      0              0.000000  \n",
      "1029    121              0.000070  \n",
      "1030    261              0.000151  \n",
      "1031     18              0.000010  \n",
      "\n",
      "[1032 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "word_list = [\n",
    "    \"Democracy\", \"Dictatorship\",\n",
    "    \"Polyarchy\", \"Autocracy\",\n",
    "    \"Pluralism\", \"Authoritarianism\",\n",
    "    \"Egalitarian\", \"Populism\",\n",
    "    \"Consensual\", \"Erosion\",\n",
    "    \"Deliberative\", \"Coup\",\n",
    "    \"Participatory\", \"Fascism\",\n",
    "    \"Liberal\", \"Conservatism\",\n",
    "    \"Feminism\", \"Backsliding\",\n",
    "    \"Social democracy\", \"Totalitarianism\",\n",
    "    \"Representative\", \"Oligarchy\",\n",
    "    \n",
    "    # Electoral Principle of Democracy\n",
    "    \"Directly elected\", \"Appointed\", \"Appointment\",\n",
    "    \"Electoral competition\", \"Electoral authoritarianism\",\n",
    "    \"Electoral integrity\", \"Electoral fraud\",\n",
    "    \"Multiparty\", \"Multi party\", \"Single party\",\n",
    "    \"Free speech\", \"Censorship\",\n",
    "    \"Free press\", \"Propaganda\",\n",
    "    \n",
    "    # Liberal Principle of Democracy\n",
    "    \"Independent judiciary\", \"State intervention\",\n",
    "    \"Legislative oversight\", \"Legislative supremacy\",\n",
    "    \"Transparent\", \"Transparency\", \"Corrupt\", \"Corruption\",\n",
    "    \"Civil liberties\", \"State repression\",\n",
    "    \"Privacy\", \"Surveillance\",\n",
    "    \n",
    "    # Deliberative Principle of Democracy\n",
    "    \"Discretion\", \"Emotional appeals\",\n",
    "    \"Rationale\", \"Hate speech\",\n",
    "    \"Common good\", \"Self interest\",\n",
    "    \"Ideological diversity\", \"Polarization\",\n",
    "    \"Consensus\", \"Coercion\",\n",
    "    \n",
    "    # Participatory Principle of Democracy\n",
    "    \"Suffrage\", \"Disenfranchisement\",\n",
    "    \"Apportionment\", \"Malapportionment\",\n",
    "    \"Direct democracy\", \"Bureaucratic politics\",\n",
    "    \"Politically active\", \"Apolitical\",\n",
    "    \"Civic engagement\", \"Apathy\", \n",
    "    \"Mass participation\", \"Indifferent\",\n",
    "    \"Decentralize\", \"Decentralized\", \"Decentralization\",\n",
    "    \"Centralize\", \"Centralized\", \"Centralization\",\n",
    "    \n",
    "    # Egalitarian Principle of Democracy\n",
    "    \"Equality\",\"Inequality\",\n",
    "    \"Minorities\", \"Ruling elite\",\n",
    "    \"Ethnic minorities\", \"Racial discrimination\",\n",
    "    \"Gender equality\", \"Discrimination\",\n",
    "    \"Redistribution\", \"Income inequality\"\n",
    "]\n",
    "\n",
    "subcorpus_folder = '/Users/yvette/Desktop/data/Final/preprocessed grouped txt'\n",
    "token_counts_df = pd.read_csv('/Users/yvette/Desktop/data/Final/group_token_counts.csv')\n",
    "result = compute_word_frequencies_from_texts(word_list, subcorpus_folder, token_counts_df)\n",
    "print(result)\n",
    "result.to_csv('/Users/yvette/Desktop/data/Final/word_frequencies.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     word                                        subcorpus  count  \\\n",
      "0  Apathy  1971-1980_American Journal of Political Science     30   \n",
      "1  Apathy   1971-1980_British Journal of Political Science     46   \n",
      "2  Apathy  1981-1990_American Journal of Political Science     12   \n",
      "3  Apathy   1981-1990_British Journal of Political Science     19   \n",
      "4  Apathy  1991-2000_American Journal of Political Science     30   \n",
      "\n",
      "   normalized_frequency  \n",
      "0              0.000017  \n",
      "1              0.000032  \n",
      "2              0.000005  \n",
      "3              0.000013  \n",
      "4              0.000010  \n"
     ]
    }
   ],
   "source": [
    "### combine word variations\n",
    "# Define mapping for word variations\n",
    "word_mapping = {\n",
    "    # \"Directly elected\": [\"Directly elected\", \"Election\"],\n",
    "    \"Appointed\": [\"Appointed\", \"Appointment\"],\n",
    "    \"Transparency\": [\"Transparent\", \"Transparency\"],\n",
    "    \"Corruption\": [\"Corrupt\", \"Corruption\"],\n",
    "    \"Decentralization\": [\"Decentralize\", \"Decentralized\", \"Decentralization\"],\n",
    "    \"Centralization\": [\"Centralize\", \"Centralized\", \"Centralization\"],\n",
    "    #\"Equality\": [\"Equal\", \"Equally\", \"Equality\"],\n",
    "    #\"Inequality\": [\"Inequal\", \"Unequally\", \"Inequality\"],\n",
    "    \"Equal right\": [\"Equal right\", \"Equal rights\"],\n",
    "    \"Multiparty\": [\"Multiparty\", \"Multi party\"],\n",
    "    # \"Ruling elite\": [\"Ruling elite\", \"Ruling elites\"]\n",
    "}\n",
    "\n",
    "# Create a new column with the standardized word\n",
    "result[\"word_standardized\"] = result[\"word\"].replace(\n",
    "    {v: k for k, values in word_mapping.items() for v in values}\n",
    ")\n",
    "\n",
    "# Group by standardized words and subcorpus, summing count and normalized frequency\n",
    "df_grouped = result.groupby([\"word_standardized\", \"subcorpus\"], as_index=False).agg(\n",
    "    {\"count\": \"sum\", \"normalized_frequency\": \"sum\"}\n",
    ")\n",
    "\n",
    "# Rename column back to \"word\" for clarity\n",
    "df_grouped.rename(columns={\"word_standardized\": \"word\"}, inplace=True)\n",
    "\n",
    "# Save the cleaned dataset\n",
    "df_grouped.to_csv(\"combined_word_frequencies.csv\", index=False)\n",
    "\n",
    "# Display result\n",
    "print(df_grouped.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Descriptive statistics\n",
    "\n",
    "# Define word groupings\n",
    "word_groups = {\n",
    "    \"Democracy vs. Dictatorship\": [\n",
    "        \"Democracy\", \"Dictatorship\", \"Polyarchy\", \"Autocracy\", \"Pluralism\", \"Authoritarianism\",\n",
    "        \"Egalitarian\", \"Populism\", \"Consensual\", \"Erosion\", \"Deliberative\", \"Coup\",\n",
    "        \"Participatory\", \"Fascism\", \"Liberal\", \"Conservatism\", \"Feminism\", \"Backsliding\",\n",
    "        \"Social democracy\", \"Totalitarianism\", \"Representative\", \"Oligarchy\"\n",
    "    ],\n",
    "    \"Electoral Principle of Democracy\": [\n",
    "        \"Directly elected\", \"Appointed\", \"Appointment\", \"Electoral competition\",\n",
    "        \"Electoral authoritarianism\", \"Electoral integrity\", \"Electoral fraud\", \"Multiparty\",\n",
    "        \"Multi party\", \"Single party\", \"Free speech\", \"Censorship\", \"Free press\", \"Propaganda\"\n",
    "    ],\n",
    "    \"Liberal Principle of Democracy\": [\n",
    "        \"Independent judiciary\", \"State intervention\", \"Legislative oversight\",\n",
    "        \"Legislative supremacy\", \"Transparent\", \"Transparency\", \"Corrupt\", \"Corruption\",\n",
    "        \"Civil liberties\", \"State repression\", \"Privacy\", \"Surveillance\"\n",
    "    ],\n",
    "    \"Deliberative Principle of Democracy\": [\n",
    "        \"Discretion\", \"Emotional appeals\", \"Rationale\", \"Hate speech\", \"Common good\",\n",
    "        \"Self interest\", \"Ideological diversity\", \"Polarization\", \"Consensus\", \"Coercion\"\n",
    "    ],\n",
    "    \"Participatory Principle of Democracy\": [\n",
    "        \"Suffrage\", \"Disenfranchisement\", \"Apportionment\", \"Malapportionment\",\n",
    "        \"Direct democracy\", \"Bureaucratic politics\", \"Politically active\", \"Apolitical\",\n",
    "        \"Civic engagement\", \"Apathy\", \"Mass participation\",\"Indifferent\", \"Decentralize\", \"Decentralized\",\n",
    "        \"Decentralization\", \"Centralize\", \"Centralized\", \"Centralization\"\n",
    "    ],\n",
    "    \"Egalitarian Principle of Democracy\": [\n",
    "        \"Equality\", \"Inequality\", \"Minorities\", \"Ruling elite\",\n",
    "        \"Ethnic minorities\", \"Racial discrimination\", \"Gender equality\",\"Discrimination\", \n",
    "        \"Redistribution\", \"Income inequality\", \"Equal right\", \"Equal rights\",\"Privilege\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Load dataset\n",
    "csv_file = \"/Users/yvette/Desktop/data/Final/combined_word_frequencies.csv\"  # Placeholder, replace with actual file path\n",
    "df = pd.read_csv(csv_file)\n",
    "\n",
    "# Standardize journal labels\n",
    "df[\"Journal\"] = df[\"subcorpus\"].apply(lambda x: \"American\" if \"American\" in x else \"British\")\n",
    "\n",
    "# Scale normalized frequency\n",
    "df[\"scaled_frequency\"] = df[\"normalized_frequency\"] * 1000  # Convert to per 1,000 tokens\n",
    "\n",
    "# Initialize results\n",
    "stats_data = []\n",
    "\n",
    "# Compute descriptive statistics per dimension and journal\n",
    "for dimension, words in word_groups.items():\n",
    "    for journal in [\"American\", \"British\"]:\n",
    "        subset = df[(df[\"word\"].isin(words)) & (df[\"Journal\"] == journal)]\n",
    "        total_frequency = subset[\"scaled_frequency\"].sum()\n",
    "        \n",
    "        if not subset.empty:\n",
    "            stats_data.append({\n",
    "                \"Dimension\": dimension,\n",
    "                \"Journal\": journal,\n",
    "                \"Mean\": subset[\"scaled_frequency\"].mean(),\n",
    "                \"SD\": subset[\"scaled_frequency\"].std(),\n",
    "                \"Min\": subset[\"scaled_frequency\"].min(),\n",
    "                \"Max\": subset[\"scaled_frequency\"].max(),\n",
    "                \"Total Frequency\": total_frequency\n",
    "            })\n",
    "\n",
    "# Convert to DataFrame\n",
    "stats_df = pd.DataFrame(stats_data)\n",
    "stats_df[[\"Mean\", \"SD\", \"Min\", \"Max\", \"Total Frequency\"]] = stats_df[[\"Mean\", \"SD\", \"Min\", \"Max\", \"Total Frequency\"]].round(3)\n",
    "stats_df.to_csv(\"/Users/yvette/Desktop/data/Final/word_frequency_descriptive_statistics.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "def plot_all_words(csv_file, output_folder):\n",
    "    \"\"\"\n",
    "    Plot the word frequency over the decades for all words in the dataset,\n",
    "    distinguishing American and British journals.\n",
    "    \n",
    "    Parameters:\n",
    "    - csv_file (str): Path to the CSV file containing 'word', 'subcorpus', 'count', 'normalized_frequency'.\n",
    "    - output_folder (str): Folder to save the generated plots.\n",
    "    \"\"\"\n",
    "    # Load the dataset\n",
    "    df = pd.read_csv(csv_file)\n",
    "    \n",
    "    # Add a column to identify the journal (American or British)\n",
    "    df['Journal'] = df['subcorpus'].apply(lambda x: 'American' if 'American' in x else 'British')\n",
    "    \n",
    "    # Add a 'Decade' column for grouping by decades\n",
    "    df['Decade'] = df['subcorpus'].apply(lambda x: x.split('_')[0][:4] + 's')\n",
    "    \n",
    "    # Get all unique words\n",
    "    words = df['word'].unique()\n",
    "    \n",
    "    # Define colors for American and British journals (culturally meaningful)\n",
    "    palette = {'American': '#005AB5', 'British': '#DC3220'}  # Deep Blue for U.S., Deep Red for UK\n",
    "    \n",
    "    # Ensure output folder exists\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "    \n",
    "    for word in words:\n",
    "        # Filter for the specific word\n",
    "        word_df = df[df['word'] == word]\n",
    "        \n",
    "        # Aggregate normalized frequency by Decade and Journal\n",
    "        aggregated_df = word_df.groupby(['Decade', 'Journal'], as_index=False)['normalized_frequency'].sum()\n",
    "        \n",
    "        # Plotting\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        sns.lineplot(data=aggregated_df, x='Decade', y='normalized_frequency', hue='Journal', \n",
    "                     style='Journal', markers=True, dashes=False, palette=palette)\n",
    "        \n",
    "        # Customize the plot\n",
    "        plt.title(f'\"{word}\" Frequency Across Decades')\n",
    "        plt.xlabel('Decade')\n",
    "        plt.ylabel('Word Frequency')\n",
    "        plt.xticks(rotation=45)\n",
    "        \n",
    "        # Save the plot\n",
    "        plot_path = os.path.join(output_folder, f\"{word.replace(' ', '_')}.png\")\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(plot_path)\n",
    "        plt.close()\n",
    "\n",
    "# Example usage\n",
    "plot_all_words(\"/Users/yvette/Desktop/data/Final/combined_word_frequencies.csv\", \"output_plots_word_frequency\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "import math\n",
    "import matplotlib.gridspec as gridspec\n",
    "from matplotlib.lines import Line2D  \n",
    "\n",
    "# Define the anchor words based on Table 2 from the document\n",
    "# This dictionary maps each dimension to its positive and negative anchor words.\n",
    "anchor_words_map = {\n",
    "    'Electoral': {\n",
    "        'Positive': [\"Directly elected\", \"Electoral competition\", \"Electoral integrity\", \"Multiparty\", \"Free speech\", \"Free press\"],\n",
    "        'Negative': [\"Appointed\", \"Electoral authoritarianism\", \"Electoral fraud\", \"Single party\", \"Censorship\", \"Propaganda\"]\n",
    "    },\n",
    "    'Liberal': {\n",
    "        'Positive': [\"Independent judiciary\", \"Legislative oversight\", \"Civil liberties\", \"Privacy\", \"Transparency\"],\n",
    "        'Negative': [\"State intervention\", \"Legislative supremacy\", \"State repression\", \"Surveillance\", \"Corruption\"]\n",
    "    },\n",
    "    'Deliberative': {\n",
    "        'Positive': [\"Discretion\", \"Rationale\", \"Common good\", \"Ideological diversity\", \"Consensus\"],\n",
    "        'Negative': [\"Emotional appeals\", \"Hate speech\", \"Self interest\", \"Polarization\", \"Coercion\"]\n",
    "    },\n",
    "    'Participatory': {\n",
    "        'Positive': [\"Suffrage\", \"Apportionment\", \"Direct democracy\", \"Politically active\", \"Civic engagement\", \"Mass participation\", \"Decentralization\"],\n",
    "        'Negative': [\"Disenfranchisement\", \"Malapportionment\", \"Bureaucratic politics\", \"Apolitical\", \"Apathy\", \"Indifferent\", \"Centralization\"]\n",
    "    },\n",
    "    'Egalitarian': {\n",
    "        'Positive': [\"Equality\", \"Minorities\", \"Ethnic minorities\", \"Gender equality\", \"Redistribution\"],\n",
    "        'Negative': [\"Inequality\", \"Ruling elite\", \"Racial discrimination\", \"Discrimination\", \"Income inequality\"]\n",
    "    },\n",
    "    'Democracy-Authoritarian': {\n",
    "        'Positive': [\"Democracy\", \"Polyarchy\", \"Pluralism\", \"Egalitarian\", \"Consensual\", \"Deliberative\", \"Participatory\", \"Liberal\", \"Feminism\", \"Social democracy\", \"Representative\"],\n",
    "        'Negative': [\"Dictatorship\", \"Autocracy\", \"Authoritarianism\", \"Populism\", \"Erosion\", \"Coup\", \"Fascism\", \"Conservatism\", \"Backsliding\", \"Totalitarianism\", \"Oligarchy\"]\n",
    "    }\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_anchor_word_optimized(csv_file, output_folder, anchor_words_map):\n",
    "    \"\"\"\n",
    "    Create optimized plots for word frequency analysis with specific requirements:\n",
    "    1. Detailed view only (no aggregated or summary views)\n",
    "    2. Positive words on left column, negative words on right column\n",
    "    3. Same y-axis scale within each dimension\n",
    "    4. No grid lines\n",
    "    5. Y-axis label centered\n",
    "    6. No legend on first graph\n",
    "    7. 4 columns for Democracy-Authoritarian dimension\n",
    "    \"\"\"\n",
    "    \n",
    "\n",
    "    # Load and preprocess data \n",
    "    df = pd.read_csv(csv_file)\n",
    "    df['Journal'] = df['subcorpus'].apply(lambda x: 'American' if 'American' in x else 'British')\n",
    "    df['Decade'] = df['subcorpus'].apply(lambda x: x.split('_')[0])\n",
    "    \n",
    "    # Sort decades chronologically (assuming format like \"1970s\")\n",
    "    decades = sorted(df['Decade'].unique(), key=lambda x: int(x[:4]))\n",
    "    \n",
    "    # Filter and map anchor words\n",
    "    all_anchor_words = [word for dim in anchor_words_map.values() \n",
    "                       for pol in dim.values() for word in pol]\n",
    "    anchor_df = df[df['word'].isin(all_anchor_words)].copy()\n",
    "    \n",
    "    # Add dimension and polarity columns\n",
    "    for dim, pol_dict in anchor_words_map.items():\n",
    "        for pol, words in pol_dict.items():\n",
    "            mask = anchor_df['word'].isin(words)\n",
    "            anchor_df.loc[mask, 'Dimension'] = dim\n",
    "            anchor_df.loc[mask, 'Polarity'] = pol\n",
    "\n",
    "    # Convert frequencies to per thousand format\n",
    "    anchor_df['frequency_per_thousand'] = anchor_df['normalized_frequency']\n",
    "    \n",
    "    # Visualization parameters\n",
    "    journal_palette = {'American': '#005AB5', 'British': '#DC3220'}\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "    \n",
    "    # Create custom legend elements\n",
    "    custom_lines = [\n",
    "        Line2D([0], [0], color=journal_palette['American'], lw=2, marker='o', markersize=5),\n",
    "        Line2D([0], [0], color=journal_palette['British'], lw=2, marker='o', markersize=5)\n",
    "    ]\n",
    "    \n",
    "    # Iterate through dimensions\n",
    "    for dimension in anchor_words_map.keys():\n",
    "        dim_data = anchor_df[anchor_df['Dimension'] == dimension].copy()\n",
    "        \n",
    "        # Get positive and negative words for this dimension\n",
    "        pos_words = anchor_words_map[dimension]['Positive']\n",
    "        neg_words = anchor_words_map[dimension]['Negative']\n",
    "        \n",
    "        # Get max y value for consistent scaling across all subplots in this dimension\n",
    "        y_max = dim_data['frequency_per_thousand'].max() * 1.1\n",
    "        \n",
    "        # Special case for Democracy-Authoritarian dimension - use 4 columns\n",
    "        if dimension == \"Democracy-Authoritarian\":\n",
    "            # Calculate number of rows needed\n",
    "            total_words = len(pos_words) + len(neg_words)\n",
    "            num_columns = 4\n",
    "            num_rows = math.ceil(total_words / num_columns)\n",
    "            \n",
    "            # Create figure with extra space at top for the dimension title and legend\n",
    "            fig = plt.figure(figsize=(20, num_rows * 3 + 1))\n",
    "            \n",
    "            # Create grid with 4 columns\n",
    "            gs = gridspec.GridSpec(num_rows, num_columns, wspace=0.3, hspace=0.4)\n",
    "            \n",
    "            # Plot all words in 4 columns\n",
    "            # First half of the columns for positive words, second half for negative\n",
    "            pos_col_span = num_columns // 2\n",
    "            neg_col_span = num_columns - pos_col_span\n",
    "            \n",
    "            # Plot positive words\n",
    "            for i, word in enumerate(pos_words):\n",
    "                row = i // pos_col_span\n",
    "                col = i % pos_col_span\n",
    "                ax = fig.add_subplot(gs[row, col])\n",
    "                \n",
    "                plot_data = dim_data[(dim_data['word'] == word) & (dim_data['Polarity'] == 'Positive')]\n",
    "                \n",
    "                sns.lineplot(\n",
    "                    data=plot_data, \n",
    "                    x='Decade', \n",
    "                    y='frequency_per_thousand',\n",
    "                    hue='Journal', \n",
    "                    style='Journal',\n",
    "                    markers=True, \n",
    "                    dashes=False,\n",
    "                    palette=journal_palette,\n",
    "                    ax=ax,\n",
    "                    linewidth=3,\n",
    "                    legend=False  # No legend as requested\n",
    "                )\n",
    "                \n",
    "                ax.set_title(f'{word}', fontsize=20)\n",
    "                ax.set_ylim(0, y_max)  # Use consistent y-axis scale\n",
    "                ax.grid(False)  # Remove grid as requested\n",
    "                \n",
    "                # X-axis formatting\n",
    "                if row == num_rows - 1 or word == \"Social democracy\":\n",
    "                    ax.set_xlabel('')\n",
    "                    ax.tick_params(axis='x', labelsize=16, rotation=45)\n",
    "                else:\n",
    "                    ax.set_xlabel('')\n",
    "                    ax.tick_params(axis='x', labelbottom=False)\n",
    "                \n",
    "                # Y-axis formatting\n",
    "                ax.set_ylabel('')\n",
    "                ax.tick_params(axis='y', labelbottom=False)\n",
    "\n",
    "            # Add a centered y-axis label\n",
    "            fig.text(0.04, 0.5, 'Frequency', va='center', rotation='vertical', fontsize=20)\n",
    "            fig.text(0.5, 0.02, 'Year', va='center', fontsize=20)\n",
    "            \n",
    "            # Plot negative words\n",
    "            for i, word in enumerate(neg_words):\n",
    "                row = i // neg_col_span\n",
    "                col = (i % neg_col_span) + pos_col_span\n",
    "                ax = fig.add_subplot(gs[row, col])\n",
    "                \n",
    "                plot_data = dim_data[(dim_data['word'] == word) & (dim_data['Polarity'] == 'Negative')]\n",
    "                \n",
    "                sns.lineplot(\n",
    "                    data=plot_data, \n",
    "                    x='Decade', \n",
    "                    y='frequency_per_thousand',\n",
    "                    hue='Journal', \n",
    "                    style='Journal',\n",
    "                    markers=True, \n",
    "                    dashes=False,\n",
    "                    palette=journal_palette,\n",
    "                    ax=ax,\n",
    "                    linewidth=3,\n",
    "                    legend=False  # No legend\n",
    "                )\n",
    "                \n",
    "                ax.set_title(f'{word}', fontsize=20)\n",
    "                ax.set_ylim(0, y_max)  # Use consistent y-axis scale\n",
    "                ax.grid(False)  # Remove grid as requested\n",
    "                ax.set_ylabel('')  # No y-label on right columns\n",
    "                \n",
    "                # X-axis formatting\n",
    "                if row == num_rows - 1 or word == \"Totalitarianism\":\n",
    "                    ax.set_xlabel('')\n",
    "                    ax.tick_params(axis='x', labelsize=16, rotation=45)\n",
    "                else:\n",
    "                    ax.set_xlabel('')\n",
    "                    ax.tick_params(axis='x', labelbottom=False)\n",
    "            \n",
    "            # Add column titles\n",
    "            fig.text(0.28, 0.9, 'Pro Democracy', ha='center', fontsize=22)\n",
    "            fig.text(0.7, 0.9, 'Anti Democracy', ha='center', fontsize=22)\n",
    "            \n",
    "        else:\n",
    "            # Standard 2-column layout for other dimensions\n",
    "            # Create figure with extra space at top for legend\n",
    "            max_rows = max(len(pos_words), len(neg_words))\n",
    "            fig = plt.figure(figsize=(16, max_rows * 2.5 + 1))\n",
    "            \n",
    "            # Create grid with 2 columns - left for positive, right for negative words\n",
    "            gs = gridspec.GridSpec(max_rows, 2, wspace=0.2, hspace=0.4)\n",
    "            \n",
    "            # Plot positive words (left column)\n",
    "            for i, word in enumerate(pos_words):\n",
    "                ax = fig.add_subplot(gs[i, 0])\n",
    "                \n",
    "                plot_data = dim_data[(dim_data['word'] == word) & (dim_data['Polarity'] == 'Positive')]\n",
    "                \n",
    "                sns.lineplot(\n",
    "                    data=plot_data, \n",
    "                    x='Decade', \n",
    "                    y='frequency_per_thousand',\n",
    "                    hue='Journal', \n",
    "                    style='Journal',\n",
    "                    markers=True, \n",
    "                    dashes=False,\n",
    "                    palette=journal_palette,\n",
    "                    ax=ax,\n",
    "                    linewidth=3,\n",
    "                    legend=False  # No legend as requested\n",
    "                )\n",
    "                \n",
    "                ax.set_title(f'{word}', fontsize=20)\n",
    "                ax.set_ylim(0, y_max)  # Use consistent y-axis scale\n",
    "                ax.grid(False)  # Remove grid as requested\n",
    "\n",
    "                \n",
    "                # X-axis formatting\n",
    "                if i == len(pos_words) - 1:\n",
    "                    ax.set_xlabel('')\n",
    "                    ax.tick_params(axis='x', labelsize=16, rotation=45)\n",
    "                else:\n",
    "                    ax.set_xlabel('')\n",
    "                    ax.tick_params(axis='x', labelbottom=False)\n",
    "                \n",
    "                # Y-axis formatting\n",
    "                ax.set_ylabel('')\n",
    "                ax.tick_params(axis='y', labelbottom=False)\n",
    "\n",
    "            # Plot negative words (right column)\n",
    "            for i, word in enumerate(neg_words):\n",
    "                ax = fig.add_subplot(gs[i, 1])\n",
    "                \n",
    "                plot_data = dim_data[(dim_data['word'] == word) & (dim_data['Polarity'] == 'Negative')]\n",
    "                \n",
    "                sns.lineplot(\n",
    "                    data=plot_data, \n",
    "                    x='Decade', \n",
    "                    y='frequency_per_thousand',\n",
    "                    hue='Journal', \n",
    "                    style='Journal',\n",
    "                    markers=True, \n",
    "                    dashes=False,\n",
    "                    palette=journal_palette,\n",
    "                    ax=ax,\n",
    "                    linewidth=3,\n",
    "                    legend=False  # No legend\n",
    "                )\n",
    "                \n",
    "                ax.set_title(f'{word}', fontsize=20)\n",
    "                ax.set_ylim(0, y_max)  # Use consistent y-axis scale\n",
    "                ax.grid(False)  # Remove grid as requested\n",
    "\n",
    "                # Y-axis formatting\n",
    "                ax.set_ylabel('')  # No y-label on right column\n",
    "                ax.tick_params(axis='y', labelbottom=False)\n",
    "\n",
    "                # X-axis formatting\n",
    "                if i == len(neg_words) - 1:\n",
    "                    ax.set_xlabel('')\n",
    "                    ax.tick_params(axis='x', labelsize=16, rotation=45)\n",
    "                else:\n",
    "                    ax.set_xlabel('')\n",
    "                    ax.tick_params(axis='x', labelbottom=False)\n",
    "\n",
    "            # Add a centered y-axis label\n",
    "            fig.text(0.04, 0.5, 'Frequency', va='center', rotation='vertical', fontsize=20)\n",
    "            fig.text(0.5, 0.02, 'Year', va='center', fontsize=20)\n",
    "\n",
    "            # Add column titles\n",
    "            fig.text(0.28, 0.91, 'Pro Democracy', ha='center', fontsize=22)\n",
    "            fig.text(0.71, 0.91, 'Anti Democracy', ha='center', fontsize=22)\n",
    "        \n",
    "        # Add a custom legend at the top of the figure\n",
    "        fig.legend(custom_lines, ['American', 'British'], \n",
    "                   loc='upper center', \n",
    "                   bbox_to_anchor=(0.5, 0.95), \n",
    "                   ncol=2, \n",
    "                   frameon=False,\n",
    "                   prop={'size': 16},  \n",
    "                   title='Journal Type',\n",
    "                   title_fontsize=18,  \n",
    "                   handlelength=2,     \n",
    "                   handleheight=2,     \n",
    "                   borderaxespad=1.5\n",
    "                   )\n",
    "        \n",
    "\n",
    "        # Adjust layout to account for the legend\n",
    "        plt.subplots_adjust(top=0.85, bottom=0.1, left=0.1, right=0.9, hspace=0.5, wspace=0.4)\n",
    "        \n",
    "        # Save figure\n",
    "        filename = f\"{dimension.replace(' ', '_')}_analysis.png\"\n",
    "        plt.savefig(os.path.join(output_folder, filename), dpi=500, bbox_inches='tight')\n",
    "        plt.close()\n",
    "        \n",
    "        print(f\"Saved plot for {dimension} dimension\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved plot for Electoral dimension\n",
      "Saved plot for Liberal dimension\n",
      "Saved plot for Deliberative dimension\n",
      "Saved plot for Participatory dimension\n",
      "Saved plot for Egalitarian dimension\n",
      "Saved plot for Democracy-Authoritarian dimension\n"
     ]
    }
   ],
   "source": [
    "\n",
    "plot_anchor_word_optimized(\n",
    "     \"/Users/yvette/Desktop/data/Final/combined_word_frequencies.csv\",\n",
    "    \"output_plots_grouped_frequency\",\n",
    "   anchor_words_map\n",
    " )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'List of anchor words.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[78], line 19\u001b[0m\n\u001b[1;32m     11\u001b[0m colors \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAmerican Positive\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m#005AB5\u001b[39m\u001b[38;5;124m'\u001b[39m,  \u001b[38;5;66;03m# Solid blue\u001b[39;00m\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAmerican Negative\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m#6C9BD2\u001b[39m\u001b[38;5;124m'\u001b[39m,  \u001b[38;5;66;03m# Lighter blue\u001b[39;00m\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mBritish Positive\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m#DC3220\u001b[39m\u001b[38;5;124m'\u001b[39m,   \u001b[38;5;66;03m# Solid red\u001b[39;00m\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mBritish Negative\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m#E88B8B\u001b[39m\u001b[38;5;124m'\u001b[39m    \u001b[38;5;66;03m# Lighter red\u001b[39;00m\n\u001b[1;32m     16\u001b[0m }\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# Load and prepare data\u001b[39;00m\n\u001b[0;32m---> 19\u001b[0m anchor_words \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mList of anchor words.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     20\u001b[0m word_freq \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcombined_word_frequencies.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     22\u001b[0m word_freq[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mjournal\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m word_freq[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msubcorpus\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m x: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAmerican\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAmerican\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m x \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mBritish\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/io/parsers/readers.py:912\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m    899\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m    900\u001b[0m     dialect,\n\u001b[1;32m    901\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    908\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[1;32m    909\u001b[0m )\n\u001b[1;32m    910\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m--> 912\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/io/parsers/readers.py:577\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    574\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[1;32m    576\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 577\u001b[0m parser \u001b[38;5;241m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[1;32m    579\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[1;32m    580\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/io/parsers/readers.py:1407\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1404\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m   1406\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1407\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_engine(f, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/io/parsers/readers.py:1661\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1659\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[1;32m   1660\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1661\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m get_handle(\n\u001b[1;32m   1662\u001b[0m     f,\n\u001b[1;32m   1663\u001b[0m     mode,\n\u001b[1;32m   1664\u001b[0m     encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[1;32m   1665\u001b[0m     compression\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcompression\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[1;32m   1666\u001b[0m     memory_map\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmemory_map\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m),\n\u001b[1;32m   1667\u001b[0m     is_text\u001b[38;5;241m=\u001b[39mis_text,\n\u001b[1;32m   1668\u001b[0m     errors\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding_errors\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstrict\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m   1669\u001b[0m     storage_options\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstorage_options\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[1;32m   1670\u001b[0m )\n\u001b[1;32m   1671\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1672\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/io/common.py:859\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    854\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    855\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[1;32m    856\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[1;32m    857\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[1;32m    858\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[0;32m--> 859\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(\n\u001b[1;32m    860\u001b[0m             handle,\n\u001b[1;32m    861\u001b[0m             ioargs\u001b[38;5;241m.\u001b[39mmode,\n\u001b[1;32m    862\u001b[0m             encoding\u001b[38;5;241m=\u001b[39mioargs\u001b[38;5;241m.\u001b[39mencoding,\n\u001b[1;32m    863\u001b[0m             errors\u001b[38;5;241m=\u001b[39merrors,\n\u001b[1;32m    864\u001b[0m             newline\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    865\u001b[0m         )\n\u001b[1;32m    866\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    867\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[1;32m    868\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'List of anchor words.csv'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "# Set style\n",
    "sns.set_theme(style=\"white\")\n",
    "plt.style.use('default')\n",
    "\n",
    "# Define colors\n",
    "colors = {\n",
    "    'American Positive': '#005AB5',  # Solid blue\n",
    "    'American Negative': '#6C9BD2',  # Lighter blue\n",
    "    'British Positive': '#DC3220',   # Solid red\n",
    "    'British Negative': '#E88B8B'    # Lighter red\n",
    "}\n",
    "\n",
    "# Load and prepare data\n",
    "anchor_words = pd.read_csv('List of anchor words.csv')\n",
    "word_freq = pd.read_csv('combined_word_frequencies.csv')\n",
    "\n",
    "word_freq['journal'] = word_freq['subcorpus'].apply(lambda x: 'American' if 'American' in x else 'British')\n",
    "word_freq['time'] = word_freq['subcorpus'].str.extract(r'(\\d{4}-\\d{4})')\n",
    "\n",
    "# Process data\n",
    "plot_data = []\n",
    "for dim in anchor_words['dimension'].unique():\n",
    "    dim_data = anchor_words[anchor_words['dimension'] == dim]\n",
    "    pos_words = dim_data['positive'].dropna().tolist()\n",
    "    neg_words = dim_data['negative'].dropna().tolist()\n",
    "    \n",
    "    for time in word_freq['time'].unique():\n",
    "        time_data = word_freq[word_freq['time'] == time]\n",
    "        \n",
    "        for journal in ['American', 'British']:\n",
    "            journal_data = time_data[time_data['journal'] == journal]\n",
    "            pos = journal_data[journal_data['word'].isin(pos_words)]['normalized_frequency'].sum()\n",
    "            neg = journal_data[journal_data['word'].isin(neg_words)]['normalized_frequency'].sum()\n",
    "            \n",
    "            plot_data.append({\n",
    "                'Dimension': dim,\n",
    "                'Time': time,\n",
    "                'Journal': journal,\n",
    "                'Positive': pos,\n",
    "                'Negative': neg,\n",
    "                'Total': pos + neg\n",
    "            })\n",
    "\n",
    "df = pd.DataFrame(plot_data)\n",
    "\n",
    "# Calculate layout - 3 rows, 2 columns\n",
    "n_rows = 3\n",
    "n_cols = 2\n",
    "fig, axs = plt.subplots(n_rows, n_cols, figsize=(16, 5*n_rows))\n",
    "axs = axs.flatten()\n",
    "\n",
    "# Find max y-value (excluding Democracy vs. Dictatorship)\n",
    "max_y = df[df['Dimension'] != \"Democracy  vs.  Dictatorship\"]['Total'].max() * 1.1\n",
    "\n",
    "# Plot each dimension\n",
    "for i, dim in enumerate(df['Dimension'].unique()):\n",
    "    ax = axs[i]\n",
    "    dim_df = df[df['Dimension'] == dim].sort_values('Time')\n",
    "    time_periods = dim_df['Time'].unique()\n",
    "    \n",
    "    # Set bar positions\n",
    "    x = np.arange(len(time_periods))\n",
    "    width = 0.35\n",
    "    \n",
    "    for j, time in enumerate(time_periods):\n",
    "        time_df = dim_df[dim_df['Time'] == time]\n",
    "        \n",
    "        # American stacked bar\n",
    "        am = time_df[time_df['Journal'] == 'American'].iloc[0]\n",
    "        ax.bar(x[j] - width/2, am['Positive'], width, \n",
    "               color=colors['American Positive'], label='Am Positive' if j==0 else \"\")\n",
    "        ax.bar(x[j] - width/2, am['Negative'], width, \n",
    "               bottom=am['Positive'],\n",
    "               color=colors['American Negative'], label='Am Negative' if j==0 else \"\")\n",
    "        \n",
    "        # British stacked bar\n",
    "        br = time_df[time_df['Journal'] == 'British'].iloc[0]\n",
    "        ax.bar(x[j] + width/2, br['Positive'], width, \n",
    "               color=colors['British Positive'], label='Br Positive' if j==0 else \"\")\n",
    "        ax.bar(x[j] + width/2, br['Negative'], width, \n",
    "               bottom=br['Positive'],\n",
    "               color=colors['British Negative'], label='Br Negative' if j==0 else \"\")\n",
    "    \n",
    "    # Formatting\n",
    "    if dim == 'Democracy  vs.  Dictatorship':\n",
    "        ax.set_title('Democracy-Authoritarian Dimension', fontsize=22, pad=15)\n",
    "    else:\n",
    "        ax.set_title(f'{dim} Dimension', fontsize=22, pad=15)\n",
    "\n",
    "    ax.set_xticks(x)\n",
    "    \n",
    "    # Only show x-axis labels for bottom row (plots 4 and 5 in 0-based index)\n",
    "    if i >= (n_rows-1)*n_cols:  # Bottom row\n",
    "        ax.set_xticklabels(time_periods, fontsize=20, rotation=45)\n",
    "        ax.set_xlabel('', fontsize=22)\n",
    "    else:\n",
    "        ax.set_xticklabels([])\n",
    "        ax.set_xlabel('')\n",
    "\n",
    "    \n",
    "    if i % n_cols == 0:  # Left column\n",
    "        ax.set_ylabel('', fontsize=22)\n",
    "    else:\n",
    "        ax.set_ylabel('')\n",
    "    \n",
    "    ax.yaxis.set_major_formatter(plt.FormatStrFormatter('%.3f'))\n",
    "    ax.tick_params(axis='y', labelsize=20)\n",
    "    \n",
    "    if dim != \"Democracy  vs.  Dictatorship\":\n",
    "        ax.set_ylim(0, max_y)\n",
    "    \n",
    "    # Clean spines\n",
    "    for spine in ['top', 'right']:\n",
    "        ax.spines[spine].set_visible(False)\n",
    "\n",
    "\n",
    "# Hide empty subplots\n",
    "for j in range(i+1, len(axs)):\n",
    "    axs[j].axis('off')\n",
    "\n",
    "# Create unified legend\n",
    "legend_elements = [\n",
    "    plt.Rectangle((0,0),1,1, color=colors['American Positive'], label='American Positive'),\n",
    "    plt.Rectangle((0,0),1,1, color=colors['American Negative'], label='American Negative'),\n",
    "    plt.Rectangle((0,0),1,1, color=colors['British Positive'], label='British Positive'),\n",
    "    plt.Rectangle((0,0),1,1, color=colors['British Negative'], label='British Negative')\n",
    "]\n",
    "\n",
    "fig.legend(handles=legend_elements, \n",
    "           loc='lower center',\n",
    "           bbox_to_anchor=(0.5, 1),\n",
    "           ncol=4, fontsize=22)\n",
    "\n",
    "# Final layout\n",
    "plt.tight_layout()\n",
    "fig.subplots_adjust(bottom=0.10)\n",
    "plt.savefig('word_frequency.png', dpi=500, bbox_inches='tight')\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
