{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import random \n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models.phrases import Phraser\n",
    "import os\n",
    "from nltk.tokenize import word_tokenize\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## bootstrapped the article data\n",
    "\n",
    "def write_booted_txt(input_file, output_file, seed_no):\n",
    "\n",
    "    # Read the original text data\n",
    "    with open(input_file, \"r\", encoding=\"utf-8\") as file:\n",
    "        lines = file.readlines()  # Read the file line by line\n",
    "\n",
    "    if not lines:\n",
    "        print(f\"âš  No content found in {input_file}, skipping.\")\n",
    "        return\n",
    "\n",
    "    # Write the new bootstrapped text file\n",
    "    with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "\n",
    "        random.seed(seed_no)\n",
    "        bootstrapped_lines = random.choices(lines, k=len(lines))  # Sampling with replacement\n",
    "\n",
    "        for article in bootstrapped_lines:\n",
    "            # Split the article into sentences using the delimiter\n",
    "            sentences_list = article.split(' SENTENCESPLITHERE ')  # Split into sentences\n",
    "            \n",
    "            # Write each sentence to the file, ensuring each sentence is on a new line\n",
    "            for sent in sentences_list:\n",
    "                if sent.strip():  # Avoid writing empty sentences\n",
    "                    f.write(sent.strip())  # Write sentence without leading/trailing spaces\n",
    "                    f.write(\"\\n\")  # Add newline after each sentence\n",
    "    \n",
    "input_file = '/Users/yvette/Desktop/data/Final/preprocessed grouped txt'\n",
    "output_file = '/Users/yvette/Desktop/data/Final/bootstrap txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentenceIterator:   \n",
    "    def __init__(self, filepath): \n",
    "        self.filepath = filepath\n",
    "\n",
    "    def __iter__(self): \n",
    "        for line in open(self.filepath, \"r\", encoding=\"utf-8\" ): \n",
    "            yield word_tokenize(line.rstrip('\\n'))\n",
    "\n",
    "           \n",
    "class PhrasingIterable(object):\n",
    "    def __init__(self, phrasifier, texts):\n",
    "        self.phrasifier, self.texts = phrasifier, texts\n",
    "    def __iter__(self):\n",
    "        return iter(self.phrasifier[self.texts])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the time window year ranges as per your filenames\n",
    "year_ranges = [\n",
    "    (\"1971\", \"1980\"),\n",
    "    (\"1981\", \"1990\"),\n",
    "    (\"1991\", \"2000\"),\n",
    "    (\"2001\", \"2010\"),\n",
    "    (\"2011\", \"2020\"),\n",
    "    (\"2021\", \"2024\")\n",
    "]\n",
    "\n",
    "# Define the journals\n",
    "journals = [\n",
    "    \"American Journal of Political Science\",\n",
    "    \"British Journal of Political Science\"\n",
    "]\n",
    "\n",
    "# Loop over each year range and journal to process subcorpus\n",
    "for start_year, end_year in year_ranges:\n",
    "    for journal in journals:\n",
    "        # Create the subcorpus name (e.g., 1971-1980_American Journal of Political Science)\n",
    "        subcorpus_name = f\"{start_year}-{end_year}_{journal}\"\n",
    "\n",
    "        print(f\"Processing subcorpus: {subcorpus_name}...\")\n",
    "\n",
    "        # Define the paths for bigram models\n",
    "        bigram_model_path = f\"/Users/yvette/Desktop/data/Final/bigram phraser full text/bigram_text_{subcorpus_name}.model\"\n",
    "        \n",
    "\n",
    "        # Load the bigram models\n",
    "        bigram_transformer = Phraser.load(bigram_model_path)\n",
    "        \n",
    "\n",
    "        # Loop for 25 bootstraps for each subcorpus\n",
    "        for boot in range(25):\n",
    "            print(f\"Processing bootstrap {boot} for {subcorpus_name}...\")\n",
    "\n",
    "            # Write bootstrapped text\n",
    "            input_file = f'/Users/yvette/Desktop/data/Final/preprocessed grouped txt/text_{subcorpus_name}.txt'\n",
    "            output_file = f'/Users/yvette/Desktop/data/Final/bootstrap txt/bootstrapped_text_{subcorpus_name}_boot{boot}.txt'\n",
    "            write_booted_txt(input_file, output_file, seed_no=boot)\n",
    "\n",
    "            # Process the bootstrapped file\n",
    "            bootstrapped_file = f'/Users/yvette/Desktop/data/Final/bootstrap txt/bootstrapped_text_{subcorpus_name}_boot{boot}.txt'\n",
    "            sentences = SentenceIterator(bootstrapped_file)\n",
    "\n",
    "            # Apply bigram transformation to the sentences\n",
    "            corpus = PhrasingIterable(bigram_transformer, sentences)\n",
    "\n",
    "            # Train the Word2Vec model\n",
    "            model = Word2Vec(corpus, sg=1, vector_size=300, window=10, min_count=10, workers=10, hs=0, negative=15, epochs=10)\n",
    "            model.init_sims(replace=True)\n",
    "\n",
    "            # Save the trained Word2Vec model\n",
    "            model_save_path = f\"/Users/yvette/Desktop/data/Final/bootstrapped_model_bigram/sg300win10c10iter10_{subcorpus_name}_boot{boot}_.model\"\n",
    "            model.save(model_save_path)\n",
    "\n",
    "            # Allow a short pause between model saves\n",
    "            time.sleep(120)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Model evaluation\n",
    "# Path to models directory\n",
    "models_dir = \"/Users/yvette/Desktop/data/Final/bootstrapped_model_bigram/\"\n",
    "output_csv = \"/Users/yvette/Desktop/data/Final/word_similarities_democracy.csv\"\n",
    "\n",
    "# List all model files (ignore .npy files)\n",
    "model_files = [f for f in os.listdir(models_dir) if f.endswith(\".model\")]\n",
    "\n",
    "# Initialize list to store results\n",
    "results = []\n",
    "\n",
    "# Iterate over each model file\n",
    "for model_file in model_files:\n",
    "    model_path = os.path.join(models_dir, model_file)\n",
    "    print(f\"Loading model: {model_file}...\")\n",
    "\n",
    "    try:\n",
    "        # Load Word2Vec model\n",
    "        model = Word2Vec.load(model_path)\n",
    "        \n",
    "        # Get top 5 most similar words to \"democracy\"\n",
    "        similar_words = model.wv.most_similar(\"democracy\", topn=5)\n",
    "        top_words = [word for word, score in similar_words]\n",
    "        \n",
    "        # Extract subcorpus name from the filename\n",
    "        subcorpus_name = \"_\".join(model_file.split(\"_\")[1:3])  # Example: \"1971-1980_American Journal of Political Science\"\n",
    "        \n",
    "        # Store results\n",
    "        results.append([model_file, subcorpus_name] + top_words)\n",
    "    \n",
    "    except KeyError:\n",
    "        print(f\"Word 'democracy' not found in {model_file}, skipping...\")\n",
    "        results.append([model_file, subcorpus_name] + [\"N/A\"] * 5)\n",
    "\n",
    "# Convert results to DataFrame\n",
    "columns = [\"Model\", \"Subcorpus\", \"Top1\", \"Top2\", \"Top3\", \"Top4\", \"Top5\"]\n",
    "df = pd.DataFrame(results, columns=columns)\n",
    "\n",
    "# Save to CSV\n",
    "df.to_csv(output_csv, index=False)\n",
    "\n",
    "print(f\"Results saved to {output_csv}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
